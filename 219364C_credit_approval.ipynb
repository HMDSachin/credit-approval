{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b,30.83,0,u,g,w,v,1.25,t,t,01,f,g,00202,0,+\n",
      "a,58.67,4.46,u,g,q,h,3.04,t,t,06,f,g,00043,560,+\n",
      "a,24.50,0.5,u,g,q,h,1.5,t,f,0,f,g,00280,824,+\n",
      "b,27.83,1.54,u,g,w,v,3.75,t,t,05,t,g,00100,3,+\n",
      "b,20.17,5.625,u,g,w,v,1.71,t,f,0,f,s,00120,0,+\n",
      "b,32.08,4,u,g,m,v,2.5,t,f,0,t,g,00360,0,+\n",
      "b,33.17,1.04,u,g,r,h,6.5,t,f,0,t,g,00164,31285,+\n",
      "a,22.92,11.585,u,g,cc,v,0.04,t,f,0,f,g,00080,1349,+\n",
      "b,54.42,0.5,y,p,k,h,3.96,t,f,0,f,g,00180,314,+\n",
      "b,42.50,4.915,y,p,w,v,3.165,t,f,0,t,g,00052,1442,+\n",
      "b,22.08,0.83,u,g,c,h,2.165,f,f,0,t,g,00128,0,+\n",
      "b,29.92,1.835,u,g,c,h,4.335,t,f,0,f,g,00260,200,+\n",
      "a,38.25,6,u,g,k,v,1,t,f,0,t,g,00000,0,+\n",
      "b,48.08,6.04,u,g,k,v,0.04,f,f,0,f,g,00000,2690,+\n",
      "a,45.83,10.5,u,g,q,v,5,t,t,07,t,g,00000,0,+\n",
      "b,36.67,4.415,y,p,k,v,0.25,t,t,10,t,g,00320,0,+\n",
      "b,28.25,0.875,u,g,m,v,0.96,t,t,03,t,g,00396,0,+\n",
      "a,23.25,5.875,u,g,q,v,3.17,t,t,10,f,g,00120,245,+\n",
      "b,21.83,0.25,u,g,d,h,0.665,t,f,0,t,g,00000,0,+\n",
      "a,19.17,8.585,u,g,cc,h,0.75,t,t,07,f,g,00096,0,+\n",
      "b,25.00,11.25,u,g,c,v,2.5,t,t,17,f,g,00200,1208,+\n",
      "b,23.25,1,u,g,c,v,0.835,t,f,0,f,s,00300,0,+\n",
      "a,47.75,8,u,g,c,v,7.875,t,t,06,t,g,00000,1260,+\n",
      "a,27.42,14.5,u,g,x,h,3.085,t,t,01,f,g,00120,11,+\n",
      "a,41.17,6.5,u,g,q,v,0.5,t,t,03,t,g,00145,0,+\n",
      "a,15.83,0.585,u,g,c,h,1.5,t,t,02,f,g,00100,0,+\n",
      "a,47.00,13,u,g,i,bb,5.165,t,t,09,t,g,00000,0,+\n",
      "b,56.58,18.5,u,g,d,bb,15,t,t,17,t,g,00000,0,+\n",
      "b,57.42,8.5,u,g,e,h,7,t,t,03,f,g,00000,0,+\n",
      "b,42.08,1.04,u,g,w,v,5,t,t,06,t,g,00500,10000,+\n",
      "b,29.25,14.79,u,g,aa,v,5.04,t,t,05,t,g,00168,0,+\n",
      "b,42.00,9.79,u,g,x,h,7.96,t,t,08,f,g,00000,0,+\n",
      "b,49.50,7.585,u,g,i,bb,7.585,t,t,15,t,g,00000,5000,+\n",
      "a,36.75,5.125,u,g,e,v,5,t,f,0,t,g,00000,4000,+\n",
      "a,22.58,10.75,u,g,q,v,0.415,t,t,05,t,g,00000,560,+\n",
      "b,27.83,1.5,u,g,w,v,2,t,t,11,t,g,00434,35,+\n",
      "b,27.25,1.585,u,g,cc,h,1.835,t,t,12,t,g,00583,713,+\n",
      "a,23.00,11.75,u,g,x,h,0.5,t,t,02,t,g,00300,551,+\n",
      "b,27.75,0.585,y,p,cc,v,0.25,t,t,02,f,g,00260,500,+\n",
      "b,54.58,9.415,u,g,ff,ff,14.415,t,t,11,t,g,00030,300,+\n",
      "b,34.17,9.17,u,g,c,v,4.5,t,t,12,t,g,00000,221,+\n",
      "b,28.92,15,u,g,c,h,5.335,t,t,11,f,g,00000,2283,+\n",
      "b,29.67,1.415,u,g,w,h,0.75,t,t,01,f,g,00240,100,+\n",
      "b,39.58,13.915,u,g,w,v,8.625,t,t,06,t,g,00070,0,+\n",
      "b,56.42,28,y,p,c,v,28.5,t,t,40,f,g,00000,15,+\n",
      "b,54.33,6.75,u,g,c,h,2.625,t,t,11,t,g,00000,284,+\n",
      "a,41.00,2.04,y,p,q,h,0.125,t,t,23,t,g,00455,1236,+\n",
      "b,31.92,4.46,u,g,cc,h,6.04,t,t,03,f,g,00311,300,+\n",
      "b,41.50,1.54,u,g,i,bb,3.5,f,f,0,f,g,00216,0,+\n",
      "b,23.92,0.665,u,g,c,v,0.165,f,f,0,f,g,00100,0,+\n",
      "a,25.75,0.5,u,g,c,h,0.875,t,f,0,t,g,00491,0,+\n",
      "b,26.00,1,u,g,q,v,1.75,t,f,0,t,g,00280,0,+\n",
      "b,37.42,2.04,u,g,w,v,0.04,t,f,0,t,g,00400,5800,+\n",
      "b,34.92,2.5,u,g,w,v,0,t,f,0,t,g,00239,200,+\n",
      "b,34.25,3,u,g,cc,h,7.415,t,f,0,t,g,00000,0,+\n",
      "b,23.33,11.625,y,p,w,v,0.835,t,f,0,t,g,00160,300,+\n",
      "b,23.17,0,u,g,cc,v,0.085,t,f,0,f,g,00000,0,+\n",
      "b,44.33,0.5,u,g,i,h,5,t,f,0,t,g,00320,0,+\n",
      "b,35.17,4.5,u,g,x,h,5.75,f,f,0,t,s,00711,0,+\n",
      "b,43.25,3,u,g,q,h,6,t,t,11,f,g,00080,0,+\n",
      "b,56.75,12.25,u,g,m,v,1.25,t,t,04,t,g,00200,0,+\n",
      "b,31.67,16.165,u,g,d,v,3,t,t,09,f,g,00250,730,+\n",
      "a,23.42,0.79,y,p,q,v,1.5,t,t,02,t,g,00080,400,+\n",
      "a,20.42,0.835,u,g,q,v,1.585,t,t,01,f,g,00000,0,+\n",
      "b,26.67,4.25,u,g,cc,v,4.29,t,t,01,t,g,00120,0,+\n",
      "b,34.17,1.54,u,g,cc,v,1.54,t,t,01,t,g,00520,50000,+\n",
      "a,36.00,1,u,g,c,v,2,t,t,11,f,g,00000,456,+\n",
      "b,25.50,0.375,u,g,m,v,0.25,t,t,03,f,g,00260,15108,+\n",
      "b,19.42,6.5,u,g,w,h,1.46,t,t,07,f,g,00080,2954,+\n",
      "b,35.17,25.125,u,g,x,h,1.625,t,t,01,t,g,00515,500,+\n",
      "b,32.33,7.5,u,g,e,bb,1.585,t,f,0,t,s,00420,0,-\n",
      "a,38.58,5,u,g,cc,v,13.5,t,f,0,t,g,00980,0,-\n",
      "b,44.25,0.5,u,g,m,v,10.75,t,f,0,f,s,00400,0,-\n",
      "b,44.83,7,y,p,c,v,1.625,f,f,0,f,g,00160,2,-\n",
      "b,20.67,5.29,u,g,q,v,0.375,t,t,01,f,g,00160,0,-\n",
      "b,34.08,6.5,u,g,aa,v,0.125,t,f,0,t,g,00443,0,-\n",
      "a,19.17,0.585,y,p,aa,v,0.585,t,f,0,t,g,00160,0,-\n",
      "b,21.67,1.165,y,p,k,v,2.5,t,t,01,f,g,00180,20,-\n",
      "b,21.50,9.75,u,g,c,v,0.25,t,f,0,f,g,00140,0,-\n",
      "b,49.58,19,u,g,ff,ff,0,t,t,01,f,g,00094,0,-\n",
      "a,27.67,1.5,u,g,m,v,2,t,f,0,f,s,00368,0,-\n",
      "b,39.83,0.5,u,g,m,v,0.25,t,f,0,f,s,00288,0,-\n",
      "b,27.25,0.625,u,g,aa,v,0.455,t,f,0,t,g,00200,0,-\n",
      "b,37.17,4,u,g,c,bb,5,t,f,0,t,s,00280,0,-\n",
      "b,25.67,2.21,y,p,aa,v,4,t,f,0,f,g,00188,0,-\n",
      "b,34.00,4.5,u,g,aa,v,1,t,f,0,t,g,00240,0,-\n",
      "a,49.00,1.5,u,g,j,j,0,t,f,0,t,g,00100,27,-\n",
      "b,62.50,12.75,y,p,c,h,5,t,f,0,f,g,00112,0,-\n",
      "b,31.42,15.5,u,g,c,v,0.5,t,f,0,f,g,00120,0,-\n",
      "b,52.33,1.375,y,p,c,h,9.46,t,f,0,t,g,00200,100,-\n",
      "b,28.75,1.5,y,p,c,v,1.5,t,f,0,t,g,00000,225,-\n",
      "a,28.58,3.54,u,g,i,bb,0.5,t,f,0,t,g,00171,0,-\n",
      "b,23.00,0.625,y,p,aa,v,0.125,t,f,0,f,g,00180,1,-\n",
      "a,22.50,11,y,p,q,v,3,t,f,0,t,g,00268,0,-\n",
      "a,28.50,1,u,g,q,v,1,t,t,02,t,g,00167,500,-\n",
      "b,37.50,1.75,y,p,c,bb,0.25,t,f,0,t,g,00164,400,-\n",
      "b,35.25,16.5,y,p,c,v,4,t,f,0,f,g,00080,0,-\n",
      "b,18.67,5,u,g,q,v,0.375,t,t,02,f,g,00000,38,-\n",
      "b,25.00,12,u,g,k,v,2.25,t,t,02,t,g,00120,5,-\n",
      "b,27.83,4,y,p,i,h,5.75,t,t,02,t,g,00075,0,-\n",
      "b,54.83,15.5,u,g,e,z,0,t,t,20,f,g,00152,130,-\n",
      "b,28.75,1.165,u,g,k,v,0.5,t,f,0,f,s,00280,0,-\n",
      "a,25.00,11,y,p,aa,v,4.5,t,f,0,f,g,00120,0,-\n",
      "b,40.92,2.25,y,p,x,h,10,t,f,0,t,g,00176,0,-\n",
      "a,19.75,0.75,u,g,c,v,0.795,t,t,05,t,g,00140,5,-\n",
      "b,29.17,3.5,u,g,w,v,3.5,t,t,03,t,g,00329,0,-\n",
      "a,24.50,1.04,y,p,ff,ff,0.5,t,t,03,f,g,00180,147,-\n",
      "b,24.58,12.5,u,g,w,v,0.875,t,f,0,t,g,00260,0,-\n",
      "a,33.75,0.75,u,g,k,bb,1,t,t,03,t,g,00212,0,-\n",
      "b,20.67,1.25,y,p,c,h,1.375,t,t,03,t,g,00140,210,-\n",
      "a,25.42,1.125,u,g,q,v,1.29,t,t,02,f,g,00200,0,-\n",
      "b,37.75,7,u,g,q,h,11.5,t,t,07,t,g,00300,5,-\n",
      "b,52.50,6.5,u,g,k,v,6.29,t,t,15,f,g,00000,11202,+\n",
      "b,57.83,7.04,u,g,m,v,14,t,t,06,t,g,00360,1332,+\n",
      "a,20.75,10.335,u,g,cc,h,0.335,t,t,01,t,g,00080,50,+\n",
      "b,39.92,6.21,u,g,q,v,0.04,t,t,01,f,g,00200,300,+\n",
      "b,25.67,12.5,u,g,cc,v,1.21,t,t,67,t,g,00140,258,+\n",
      "a,24.75,12.5,u,g,aa,v,1.5,t,t,12,t,g,00120,567,+\n",
      "a,44.17,6.665,u,g,q,v,7.375,t,t,03,t,g,00000,0,+\n",
      "a,23.50,9,u,g,q,v,8.5,t,t,05,t,g,00120,0,+\n",
      "b,34.92,5,u,g,x,h,7.5,t,t,06,t,g,00000,1000,+\n",
      "b,47.67,2.5,u,g,m,bb,2.5,t,t,12,t,g,00410,2510,+\n",
      "b,22.75,11,u,g,q,v,2.5,t,t,07,t,g,00100,809,+\n",
      "b,34.42,4.25,u,g,i,bb,3.25,t,t,02,f,g,00274,610,+\n",
      "a,28.42,3.5,u,g,w,v,0.835,t,f,0,f,s,00280,0,+\n",
      "b,67.75,5.5,u,g,e,z,13,t,t,01,t,g,00000,0,+\n",
      "b,20.42,1.835,u,g,c,v,2.25,t,t,01,f,g,00100,150,+\n",
      "a,47.42,8,u,g,e,bb,6.5,t,t,06,f,g,00375,51100,+\n",
      "b,36.25,5,u,g,c,bb,2.5,t,t,06,f,g,00000,367,+\n",
      "b,32.67,5.5,u,g,q,h,5.5,t,t,12,t,g,00408,1000,+\n",
      "b,48.58,6.5,u,g,q,h,6,t,f,0,t,g,00350,0,+\n",
      "b,39.92,0.54,y,p,aa,v,0.5,t,t,03,f,g,00200,1000,+\n",
      "b,33.58,2.75,u,g,m,v,4.25,t,t,06,f,g,00204,0,+\n",
      "a,18.83,9.5,u,g,w,v,1.625,t,t,06,t,g,00040,600,+\n",
      "a,26.92,13.5,u,g,q,h,5,t,t,02,f,g,00000,5000,+\n",
      "a,31.25,3.75,u,g,cc,h,0.625,t,t,09,t,g,00181,0,+\n",
      "a,56.50,16,u,g,j,ff,0,t,t,15,f,g,00000,247,+\n",
      "b,43.00,0.29,y,p,cc,h,1.75,t,t,08,f,g,00100,375,+\n",
      "b,22.33,11,u,g,w,v,2,t,t,01,f,g,00080,278,+\n",
      "b,27.25,1.665,u,g,cc,h,5.085,t,t,09,f,g,00399,827,+\n",
      "b,32.83,2.5,u,g,cc,h,2.75,t,t,06,f,g,00160,2072,+\n",
      "b,23.25,1.5,u,g,q,v,2.375,t,t,03,t,g,00000,582,+\n",
      "a,40.33,7.54,y,p,q,h,8,t,t,14,f,g,00000,2300,+\n",
      "a,30.50,6.5,u,g,c,bb,4,t,t,07,t,g,00000,3065,+\n",
      "a,52.83,15,u,g,c,v,5.5,t,t,14,f,g,00000,2200,+\n",
      "a,46.67,0.46,u,g,cc,h,0.415,t,t,11,t,g,00440,6,+\n",
      "a,58.33,10,u,g,q,v,4,t,t,14,f,g,00000,1602,+\n",
      "b,37.33,6.5,u,g,m,h,4.25,t,t,12,t,g,00093,0,+\n",
      "b,23.08,2.5,u,g,c,v,1.085,t,t,11,t,g,00060,2184,+\n",
      "b,32.75,1.5,u,g,cc,h,5.5,t,t,03,t,g,00000,0,+\n",
      "a,21.67,11.5,y,p,j,j,0,t,t,11,t,g,00000,0,+\n",
      "a,28.50,3.04,y,p,x,h,2.54,t,t,01,f,g,00070,0,+\n",
      "a,68.67,15,u,g,e,z,0,t,t,14,f,g,00000,3376,+\n",
      "b,28.00,2,u,g,k,h,4.165,t,t,02,t,g,00181,0,+\n",
      "b,34.08,0.08,y,p,m,bb,0.04,t,t,01,t,g,00280,2000,+\n",
      "b,27.67,2,u,g,x,h,1,t,t,04,f,g,00140,7544,+\n",
      "b,44.00,2,u,g,m,v,1.75,t,t,02,t,g,00000,15,+\n",
      "b,25.08,1.71,u,g,x,v,1.665,t,t,01,t,g,00395,20,+\n",
      "b,32.00,1.75,y,p,e,h,0.04,t,f,0,t,g,00393,0,+\n",
      "a,60.58,16.5,u,g,q,v,11,t,f,0,t,g,00021,10561,+\n",
      "a,40.83,10,u,g,q,h,1.75,t,f,0,f,g,00029,837,+\n",
      "b,19.33,9.5,u,g,q,v,1,t,f,0,t,g,00060,400,+\n",
      "a,32.33,0.54,u,g,cc,v,0.04,t,f,0,f,g,00440,11177,+\n",
      "b,36.67,3.25,u,g,q,h,9,t,f,0,t,g,00102,639,+\n",
      "b,37.50,1.125,y,p,d,v,1.5,f,f,0,t,g,00431,0,+\n",
      "a,25.08,2.54,y,p,aa,v,0.25,t,f,0,t,g,00370,0,+\n",
      "b,41.33,0,u,g,c,bb,15,t,f,0,f,g,00000,0,+\n",
      "b,56.00,12.5,u,g,k,h,8,t,f,0,t,g,00024,2028,+\n",
      "a,49.83,13.585,u,g,k,h,8.5,t,f,0,t,g,00000,0,+\n",
      "b,22.67,10.5,u,g,q,h,1.335,t,f,0,f,g,00100,0,+\n",
      "b,27.00,1.5,y,p,w,v,0.375,t,f,0,t,g,00260,1065,+\n",
      "b,25.00,12.5,u,g,aa,v,3,t,f,0,t,s,00020,0,+\n",
      "a,26.08,8.665,u,g,aa,v,1.415,t,f,0,f,g,00160,150,+\n",
      "a,18.42,9.25,u,g,q,v,1.21,t,t,04,f,g,00060,540,+\n",
      "b,20.17,8.17,u,g,aa,v,1.96,t,t,14,f,g,00060,158,+\n",
      "b,47.67,0.29,u,g,c,bb,15,t,t,20,f,g,00000,15000,+\n",
      "a,21.25,2.335,u,g,i,bb,0.5,t,t,04,f,s,00080,0,+\n",
      "a,20.67,3,u,g,q,v,0.165,t,t,03,f,g,00100,6,+\n",
      "a,57.08,19.5,u,g,c,v,5.5,t,t,07,f,g,00000,3000,+\n",
      "a,22.42,5.665,u,g,q,v,2.585,t,t,07,f,g,00129,3257,+\n",
      "b,48.75,8.5,u,g,c,h,12.5,t,t,09,f,g,00181,1655,+\n",
      "b,40.00,6.5,u,g,aa,bb,3.5,t,t,01,f,g,00000,500,+\n",
      "b,40.58,5,u,g,c,v,5,t,t,07,f,g,00000,3065,+\n",
      "a,28.67,1.04,u,g,c,v,2.5,t,t,05,t,g,00300,1430,+\n",
      "a,33.08,4.625,u,g,q,h,1.625,t,t,02,f,g,00000,0,+\n",
      "b,21.33,10.5,u,g,c,v,3,t,f,0,t,g,00000,0,+\n",
      "b,42.00,0.205,u,g,i,h,5.125,t,f,0,f,g,00400,0,+\n",
      "b,41.75,0.96,u,g,x,v,2.5,t,f,0,f,g,00510,600,+\n",
      "b,22.67,1.585,y,p,w,v,3.085,t,t,06,f,g,00080,0,+\n",
      "b,34.50,4.04,y,p,i,bb,8.5,t,t,07,t,g,00195,0,+\n",
      "b,28.25,5.04,y,p,c,bb,1.5,t,t,08,t,g,00144,7,+\n",
      "b,33.17,3.165,y,p,x,v,3.165,t,t,03,t,g,00380,0,+\n",
      "b,48.17,7.625,u,g,w,h,15.5,t,t,12,f,g,00000,790,+\n",
      "b,27.58,2.04,y,p,aa,v,2,t,t,03,t,g,00370,560,+\n",
      "b,22.58,10.04,u,g,x,v,0.04,t,t,09,f,g,00060,396,+\n",
      "a,24.08,0.5,u,g,q,h,1.25,t,t,01,f,g,00000,678,+\n",
      "a,41.33,1,u,g,i,bb,2.25,t,f,0,t,g,00000,300,+\n",
      "a,20.75,10.25,u,g,q,v,0.71,t,t,02,t,g,00049,0,+\n",
      "b,36.33,2.125,y,p,w,v,0.085,t,t,01,f,g,00050,1187,+\n",
      "a,35.42,12,u,g,q,h,14,t,t,08,f,g,00000,6590,+\n",
      "b,28.67,9.335,u,g,q,h,5.665,t,t,06,f,g,00381,168,+\n",
      "b,35.17,2.5,u,g,k,v,4.5,t,t,07,f,g,00150,1270,+\n",
      "b,39.50,4.25,u,g,c,bb,6.5,t,t,16,f,g,00117,1210,+\n",
      "b,39.33,5.875,u,g,cc,h,10,t,t,14,t,g,00399,0,+\n",
      "b,24.33,6.625,y,p,d,v,5.5,t,f,0,t,s,00100,0,+\n",
      "b,60.08,14.5,u,g,ff,ff,18,t,t,15,t,g,00000,1000,+\n",
      "b,23.08,11.5,u,g,i,v,3.5,t,t,09,f,g,00056,742,+\n",
      "b,26.67,2.71,y,p,cc,v,5.25,t,t,01,f,g,00211,0,+\n",
      "b,48.17,3.5,u,g,aa,v,3.5,t,f,0,f,s,00230,0,+\n",
      "b,41.17,4.04,u,g,cc,h,7,t,t,08,f,g,00320,0,+\n",
      "b,55.92,11.5,u,g,ff,ff,5,t,t,05,f,g,00000,8851,+\n",
      "b,53.92,9.625,u,g,e,v,8.665,t,t,05,f,g,00000,0,+\n",
      "a,18.92,9.25,y,p,c,v,1,t,t,04,t,g,00080,500,+\n",
      "a,50.08,12.54,u,g,aa,v,2.29,t,t,03,t,g,00156,0,+\n",
      "b,65.42,11,u,g,e,z,20,t,t,07,t,g,00022,0,+\n",
      "a,17.58,9,u,g,aa,v,1.375,t,f,0,t,g,00000,0,+\n",
      "a,18.83,9.54,u,g,aa,v,0.085,t,f,0,f,g,00100,0,+\n",
      "a,37.75,5.5,u,g,q,v,0.125,t,f,0,t,g,00228,0,+\n",
      "b,23.25,4,u,g,c,bb,0.25,t,f,0,t,g,00160,0,+\n",
      "b,18.08,5.5,u,g,k,v,0.5,t,f,0,f,g,00080,0,+\n",
      "a,22.50,8.46,y,p,x,v,2.46,f,f,0,f,g,00164,0,+\n",
      "b,19.67,0.375,u,g,q,v,2,t,t,02,t,g,00080,0,+\n",
      "b,22.08,11,u,g,cc,v,0.665,t,f,0,f,g,00100,0,+\n",
      "b,25.17,3.5,u,g,cc,v,0.625,t,t,07,f,g,00000,7059,+\n",
      "a,47.42,3,u,g,x,v,13.875,t,t,02,t,g,00519,1704,+\n",
      "b,33.50,1.75,u,g,x,h,4.5,t,t,04,t,g,00253,857,+\n",
      "b,27.67,13.75,u,g,w,v,5.75,t,f,0,t,g,00487,500,+\n",
      "a,58.42,21,u,g,i,bb,10,t,t,13,f,g,00000,6700,+\n",
      "a,20.67,1.835,u,g,q,v,2.085,t,t,05,f,g,00220,2503,+\n",
      "b,26.17,0.25,u,g,i,bb,0,t,f,0,t,g,00000,0,+\n",
      "b,21.33,7.5,u,g,aa,v,1.415,t,t,01,f,g,00080,9800,+\n",
      "b,42.83,4.625,u,g,q,v,4.58,t,f,0,f,s,00000,0,+\n",
      "b,38.17,10.125,u,g,x,v,2.5,t,t,06,f,g,00520,196,+\n",
      "b,20.50,10,y,p,c,v,2.5,t,f,0,f,s,00040,0,+\n",
      "b,48.25,25.085,u,g,w,v,1.75,t,t,03,f,g,00120,14,+\n",
      "b,28.33,5,u,g,w,v,11,t,f,0,t,g,00070,0,+\n",
      "b,18.50,2,u,g,i,v,1.5,t,t,02,f,g,00120,300,+\n",
      "b,33.17,3.04,y,p,c,h,2.04,t,t,01,t,g,00180,18027,+\n",
      "b,45.00,8.5,u,g,cc,h,14,t,t,01,t,g,00088,2000,+\n",
      "a,19.67,0.21,u,g,q,h,0.29,t,t,11,f,g,00080,99,+\n",
      "b,21.83,11,u,g,x,v,0.29,t,t,06,f,g,00121,0,+\n",
      "b,40.25,21.5,u,g,e,z,20,t,t,11,f,g,00000,1200,+\n",
      "b,41.42,5,u,g,q,h,5,t,t,06,t,g,00470,0,+\n",
      "a,17.83,11,u,g,x,h,1,t,t,11,f,g,00000,3000,+\n",
      "b,23.17,11.125,u,g,x,h,0.46,t,t,01,f,g,00100,0,+\n",
      "b,18.17,10.25,u,g,c,h,1.085,f,f,0,f,g,00320,13,-\n",
      "b,20.00,11.045,u,g,c,v,2,f,f,0,t,g,00136,0,-\n",
      "b,20.00,0,u,g,d,v,0.5,f,f,0,f,g,00144,0,-\n",
      "a,20.75,9.54,u,g,i,v,0.04,f,f,0,f,g,00200,1000,-\n",
      "a,24.50,1.75,y,p,c,v,0.165,f,f,0,f,g,00132,0,-\n",
      "b,32.75,2.335,u,g,d,h,5.75,f,f,0,t,g,00292,0,-\n",
      "a,52.17,0,y,p,ff,ff,0,f,f,0,f,g,00000,0,-\n",
      "a,48.17,1.335,u,g,i,o,0.335,f,f,0,f,g,00000,120,-\n",
      "a,20.42,10.5,y,p,x,h,0,f,f,0,t,g,00154,32,-\n",
      "b,50.75,0.585,u,g,ff,ff,0,f,f,0,f,g,00145,0,-\n",
      "b,17.08,0.085,y,p,c,v,0.04,f,f,0,f,g,00140,722,-\n",
      "b,18.33,1.21,y,p,e,dd,0,f,f,0,f,g,00100,0,-\n",
      "a,32.00,6,u,g,d,v,1.25,f,f,0,f,g,00272,0,-\n",
      "b,59.67,1.54,u,g,q,v,0.125,t,f,0,t,g,00260,0,+\n",
      "b,18.00,0.165,u,g,q,n,0.21,f,f,0,f,g,00200,40,+\n",
      "b,32.33,2.5,u,g,c,v,1.25,f,f,0,t,g,00280,0,-\n",
      "b,18.08,6.75,y,p,m,v,0.04,f,f,0,f,g,00140,0,-\n",
      "b,38.25,10.125,y,p,k,v,0.125,f,f,0,f,g,00160,0,-\n",
      "b,30.67,2.5,u,g,cc,h,2.25,f,f,0,t,s,00340,0,-\n",
      "b,18.58,5.71,u,g,d,v,0.54,f,f,0,f,g,00120,0,-\n",
      "a,19.17,5.415,u,g,i,h,0.29,f,f,0,f,g,00080,484,-\n",
      "a,18.17,10,y,p,q,h,0.165,f,f,0,f,g,00340,0,-\n",
      "b,16.25,0.835,u,g,m,v,0.085,t,f,0,f,s,00200,0,-\n",
      "b,21.17,0.875,y,p,c,h,0.25,f,f,0,f,g,00280,204,-\n",
      "b,23.92,0.585,y,p,cc,h,0.125,f,f,0,f,g,00240,1,-\n",
      "b,17.67,4.46,u,g,c,v,0.25,f,f,0,f,s,00080,0,-\n",
      "a,16.50,1.25,u,g,q,v,0.25,f,t,01,f,g,00108,98,-\n",
      "b,23.25,12.625,u,g,c,v,0.125,f,t,02,f,g,00000,5552,-\n",
      "b,17.58,10,u,g,w,h,0.165,f,t,01,f,g,00120,1,-\n",
      "b,29.50,0.58,u,g,w,v,0.29,f,t,01,f,g,00340,2803,-\n",
      "b,18.83,0.415,y,p,c,v,0.165,f,t,01,f,g,00200,1,-\n",
      "a,21.75,1.75,y,p,j,j,0,f,f,0,f,g,00160,0,-\n",
      "b,23.00,0.75,u,g,m,v,0.5,f,f,0,t,s,00320,0,-\n",
      "a,18.25,10,u,g,w,v,1,f,t,01,f,g,00120,1,-\n",
      "b,25.42,0.54,u,g,w,v,0.165,f,t,01,f,g,00272,444,-\n",
      "b,35.75,2.415,u,g,w,v,0.125,f,t,02,f,g,00220,1,-\n",
      "a,16.08,0.335,u,g,ff,ff,0,f,t,01,f,g,00160,126,-\n",
      "a,31.92,3.125,u,g,ff,ff,3.04,f,t,02,t,g,00200,4,-\n",
      "b,69.17,9,u,g,ff,ff,4,f,t,01,f,g,00070,6,-\n",
      "b,32.92,2.5,u,g,aa,v,1.75,f,t,02,t,g,00720,0,-\n",
      "b,16.33,2.75,u,g,aa,v,0.665,f,t,01,f,g,00080,21,-\n",
      "b,22.17,12.125,u,g,c,v,3.335,f,t,02,t,g,00180,173,-\n",
      "a,57.58,2,u,g,ff,ff,6.5,f,t,01,f,g,00000,10,-\n",
      "b,18.25,0.165,u,g,d,v,0.25,f,f,0,t,s,00280,0,-\n",
      "b,23.42,1,u,g,c,v,0.5,f,f,0,t,s,00280,0,-\n",
      "a,15.92,2.875,u,g,q,v,0.085,f,f,0,f,g,00120,0,-\n",
      "a,24.75,13.665,u,g,q,h,1.5,f,f,0,f,g,00280,1,-\n",
      "b,48.75,26.335,y,p,ff,ff,0,t,f,0,t,g,00000,0,-\n",
      "b,23.50,2.75,u,g,ff,ff,4.5,f,f,0,f,g,00160,25,-\n",
      "b,18.58,10.29,u,g,ff,ff,0.415,f,f,0,f,g,00080,0,-\n",
      "b,27.75,1.29,u,g,k,h,0.25,f,f,0,t,s,00140,0,-\n",
      "a,31.75,3,y,p,j,j,0,f,f,0,f,g,00160,20,-\n",
      "a,24.83,4.5,u,g,w,v,1,f,f,0,t,g,00360,6,-\n",
      "b,19.00,1.75,y,p,c,v,2.335,f,f,0,t,g,00112,6,-\n",
      "a,16.33,0.21,u,g,aa,v,0.125,f,f,0,f,g,00200,1,-\n",
      "a,18.58,10,u,g,d,v,0.415,f,f,0,f,g,00080,42,-\n",
      "b,16.25,0,y,p,aa,v,0.25,f,f,0,f,g,00060,0,-\n",
      "b,23.00,0.75,u,g,m,v,0.5,t,f,0,t,s,00320,0,-\n",
      "b,21.17,0.25,y,p,c,h,0.25,f,f,0,f,g,00280,204,-\n",
      "b,17.50,22,l,gg,ff,o,0,f,f,0,t,p,00450,100000,+\n",
      "b,19.17,0,y,p,m,bb,0,f,f,0,t,s,00500,1,+\n",
      "b,36.75,0.125,y,p,c,v,1.5,f,f,0,t,g,00232,113,+\n",
      "b,21.25,1.5,u,g,w,v,1.5,f,f,0,f,g,00150,8,+\n",
      "a,18.08,0.375,l,gg,cc,ff,10,f,f,0,t,s,00300,0,+\n",
      "a,33.67,0.375,u,g,cc,v,0.375,f,f,0,f,g,00300,44,+\n",
      "b,48.58,0.205,y,p,k,v,0.25,t,t,11,f,g,00380,2732,+\n",
      "b,33.67,1.25,u,g,w,v,1.165,f,f,0,f,g,00120,0,-\n",
      "a,29.50,1.085,y,p,x,v,1,f,f,0,f,g,00280,13,-\n",
      "b,30.17,1.085,y,p,c,v,0.04,f,f,0,f,g,00170,179,-\n",
      "b,34.83,2.5,y,p,w,v,3,f,f,0,f,s,00200,0,-\n",
      "a,33.25,2.5,y,p,c,v,2.5,f,f,0,t,g,00000,2,-\n",
      "b,34.08,2.5,u,g,c,v,1,f,f,0,f,g,00460,16,-\n",
      "a,25.25,12.5,u,g,d,v,1,f,f,0,t,g,00180,1062,-\n",
      "b,34.75,2.5,u,g,cc,bb,0.5,f,f,0,f,g,00348,0,-\n",
      "b,27.67,0.75,u,g,q,h,0.165,f,f,0,t,g,00220,251,-\n",
      "b,47.33,6.5,u,g,c,v,1,f,f,0,t,g,00000,228,-\n",
      "a,34.83,1.25,y,p,i,h,0.5,f,f,0,t,g,00160,0,-\n",
      "a,33.25,3,y,p,aa,v,2,f,f,0,f,g,00180,0,-\n",
      "b,28.00,3,u,g,w,v,0.75,f,f,0,t,g,00300,67,-\n",
      "a,39.08,4,u,g,c,v,3,f,f,0,f,g,00480,0,-\n",
      "b,42.75,4.085,u,g,aa,v,0.04,f,f,0,f,g,00108,100,-\n",
      "b,26.92,2.25,u,g,i,bb,0.5,f,f,0,t,g,00640,4000,-\n",
      "b,33.75,2.75,u,g,i,bb,0,f,f,0,f,g,00180,0,-\n",
      "b,38.92,1.75,u,g,k,v,0.5,f,f,0,t,g,00300,2,-\n",
      "b,62.75,7,u,g,e,z,0,f,f,0,f,g,00000,12,-\n",
      "b,26.75,4.5,y,p,c,bb,2.5,f,f,0,f,g,00200,1210,-\n",
      "b,63.33,0.54,u,g,c,v,0.585,t,t,03,t,g,00180,0,-\n",
      "b,27.83,1.5,u,g,w,v,2.25,f,t,01,t,g,00100,3,-\n",
      "a,26.17,2,u,g,j,j,0,f,f,0,t,g,00276,1,-\n",
      "b,22.17,0.585,y,p,ff,ff,0,f,f,0,f,g,00100,0,-\n",
      "b,22.50,11.5,y,p,m,v,1.5,f,f,0,t,g,00000,4000,-\n",
      "b,30.75,1.585,u,g,d,v,0.585,f,f,0,t,s,00000,0,-\n",
      "b,36.67,2,u,g,i,v,0.25,f,f,0,t,g,00221,0,-\n",
      "a,16.00,0.165,u,g,aa,v,1,f,t,02,t,g,00320,1,-\n",
      "b,41.17,1.335,u,g,d,v,0.165,f,f,0,f,g,00168,0,-\n",
      "a,19.50,0.165,u,g,q,v,0.04,f,f,0,t,g,00380,0,-\n",
      "b,32.42,3,u,g,d,v,0.165,f,f,0,t,g,00120,0,-\n",
      "a,36.75,4.71,u,g,ff,ff,0,f,f,0,f,g,00160,0,-\n",
      "a,30.25,5.5,u,g,k,v,5.5,f,f,0,t,s,00100,0,-\n",
      "b,23.08,2.5,u,g,ff,ff,0.085,f,f,0,t,g,00100,4208,-\n",
      "b,26.83,0.54,u,g,k,ff,0,f,f,0,f,g,00100,0,-\n",
      "b,16.92,0.335,y,p,k,v,0.29,f,f,0,f,s,00200,0,-\n",
      "b,24.42,2,u,g,e,dd,0.165,f,t,02,f,g,00320,1300,-\n",
      "b,42.83,1.25,u,g,m,v,13.875,f,t,01,t,g,00352,112,-\n",
      "a,22.75,6.165,u,g,aa,v,0.165,f,f,0,f,g,00220,1000,-\n",
      "b,39.42,1.71,y,p,m,v,0.165,f,f,0,f,s,00400,0,-\n",
      "a,23.58,11.5,y,p,k,h,3,f,f,0,t,g,00020,16,-\n",
      "b,21.42,0.75,y,p,r,n,0.75,f,f,0,t,g,00132,2,-\n",
      "b,33.00,2.5,y,p,w,v,7,f,f,0,t,g,00280,0,-\n",
      "b,26.33,13,u,g,e,dd,0,f,f,0,t,g,00140,1110,-\n",
      "a,45.00,4.585,u,g,k,h,1,f,f,0,t,s,00240,0,-\n",
      "b,26.25,1.54,u,g,w,v,0.125,f,f,0,f,g,00100,0,-\n",
      "a,20.83,0.5,y,p,e,dd,1,f,f,0,f,g,00260,0,-\n",
      "b,28.67,14.5,u,g,d,v,0.125,f,f,0,f,g,00000,286,-\n",
      "b,20.67,0.835,y,p,c,v,2,f,f,0,t,s,00240,0,-\n",
      "b,34.42,1.335,u,g,i,bb,0.125,f,f,0,t,g,00440,4500,-\n",
      "b,33.58,0.25,u,g,i,bb,4,f,f,0,t,s,00420,0,-\n",
      "b,43.17,5,u,g,i,bb,2.25,f,f,0,t,g,00141,0,-\n",
      "a,22.67,7,u,g,c,v,0.165,f,f,0,f,g,00160,0,-\n",
      "a,24.33,2.5,y,p,i,bb,4.5,f,f,0,f,g,00200,456,-\n",
      "a,56.83,4.25,y,p,ff,ff,5,f,f,0,t,g,00000,4,-\n",
      "b,22.08,11.46,u,g,k,v,1.585,f,f,0,t,g,00100,1212,-\n",
      "b,34.00,5.5,y,p,c,v,1.5,f,f,0,t,g,00060,0,-\n",
      "b,22.58,1.5,y,p,aa,v,0.54,f,f,0,t,g,00120,67,-\n",
      "b,21.17,0,u,g,c,v,0.5,f,f,0,t,s,00000,0,-\n",
      "b,26.67,14.585,u,g,i,bb,0,f,f,0,t,g,00178,0,-\n",
      "b,22.92,0.17,u,g,m,v,0.085,f,f,0,f,s,00000,0,-\n",
      "b,15.17,7,u,g,e,v,1,f,f,0,f,g,00600,0,-\n",
      "b,39.92,5,u,g,i,bb,0.21,f,f,0,f,g,00550,0,-\n",
      "b,27.42,12.5,u,g,aa,bb,0.25,f,f,0,t,g,00720,0,-\n",
      "b,24.75,0.54,u,g,m,v,1,f,f,0,t,g,00120,1,-\n",
      "b,41.17,1.25,y,p,w,v,0.25,f,f,0,f,g,00000,195,-\n",
      "a,33.08,1.625,u,g,d,v,0.54,f,f,0,t,g,00000,0,-\n",
      "b,29.83,2.04,y,p,x,h,0.04,f,f,0,f,g,00128,1,-\n",
      "a,23.58,0.585,y,p,ff,ff,0.125,f,f,0,f,g,00120,87,-\n",
      "b,26.17,12.5,y,p,k,h,1.25,f,f,0,t,g,00000,17,-\n",
      "b,31.00,2.085,u,g,c,v,0.085,f,f,0,f,g,00300,0,-\n",
      "b,20.75,5.085,y,p,j,v,0.29,f,f,0,f,g,00140,184,-\n",
      "b,28.92,0.375,u,g,c,v,0.29,f,f,0,f,g,00220,140,-\n",
      "a,51.92,6.5,u,g,i,bb,3.085,f,f,0,t,g,00073,0,-\n",
      "a,22.67,0.335,u,g,q,v,0.75,f,f,0,f,s,00160,0,-\n",
      "b,34.00,5.085,y,p,i,bb,1.085,f,f,0,t,g,00480,0,-\n",
      "a,69.50,6,u,g,ff,ff,0,f,f,0,f,s,00000,0,-\n",
      "a,19.58,0.665,y,p,c,v,1,f,t,01,f,g,02000,2,-\n",
      "b,16.00,3.125,u,g,w,v,0.085,f,t,01,f,g,00000,6,-\n",
      "b,17.08,0.25,u,g,q,v,0.335,f,t,04,f,g,00160,8,-\n",
      "b,31.25,2.835,u,g,ff,ff,0,f,t,05,f,g,00176,146,-\n",
      "b,25.17,3,u,g,c,v,1.25,f,t,01,f,g,00000,22,-\n",
      "a,22.67,0.79,u,g,i,v,0.085,f,f,0,f,g,00144,0,-\n",
      "b,40.58,1.5,u,g,i,bb,0,f,f,0,f,s,00300,0,-\n",
      "b,22.25,0.46,u,g,k,v,0.125,f,f,0,t,g,00280,55,-\n",
      "a,22.25,1.25,y,p,ff,ff,3.25,f,f,0,f,g,00280,0,-\n",
      "b,22.50,0.125,y,p,k,v,0.125,f,f,0,f,g,00200,70,-\n",
      "b,23.58,1.79,u,g,c,v,0.54,f,f,0,t,g,00136,1,-\n",
      "b,38.42,0.705,u,g,c,v,0.375,f,t,02,f,g,00225,500,-\n",
      "a,26.58,2.54,y,p,ff,ff,0,f,f,0,t,g,00180,60,-\n",
      "b,35.00,2.5,u,g,i,v,1,f,f,0,t,g,00210,0,-\n",
      "b,20.42,1.085,u,g,q,v,1.5,f,f,0,f,g,00108,7,-\n",
      "b,29.42,1.25,u,g,w,v,1.75,f,f,0,f,g,00200,0,-\n",
      "b,26.17,0.835,u,g,cc,v,1.165,f,f,0,f,g,00100,0,-\n",
      "b,33.67,2.165,u,g,c,v,1.5,f,f,0,f,p,00120,0,-\n",
      "b,24.58,1.25,u,g,c,v,0.25,f,f,0,f,g,00110,0,-\n",
      "a,27.67,2.04,u,g,w,v,0.25,f,f,0,t,g,00180,50,-\n",
      "b,37.50,0.835,u,g,e,v,0.04,f,f,0,f,g,00120,5,-\n",
      "b,49.17,2.29,u,g,ff,ff,0.29,f,f,0,f,g,00200,3,-\n",
      "b,33.58,0.335,y,p,cc,v,0.085,f,f,0,f,g,00180,0,-\n",
      "b,51.83,3,y,p,ff,ff,1.5,f,f,0,f,g,00180,4,-\n",
      "b,22.92,3.165,y,p,c,v,0.165,f,f,0,f,g,00160,1058,-\n",
      "b,21.83,1.54,u,g,k,v,0.085,f,f,0,t,g,00356,0,-\n",
      "b,25.25,1,u,g,aa,v,0.5,f,f,0,f,g,00200,0,-\n",
      "b,58.58,2.71,u,g,c,v,2.415,f,f,0,t,g,00320,0,-\n",
      "b,19.00,0,y,p,ff,ff,0,f,t,04,f,g,00045,1,-\n",
      "b,19.58,0.585,u,g,ff,ff,0,f,t,03,f,g,00350,769,-\n",
      "a,53.33,0.165,u,g,ff,ff,0,f,f,0,t,s,00062,27,-\n",
      "a,27.17,1.25,u,g,ff,ff,0,f,t,01,f,g,00092,300,-\n",
      "b,25.92,0.875,u,g,k,v,0.375,f,t,02,t,g,00174,3,-\n",
      "b,23.08,0,u,g,k,v,1,f,t,11,f,s,00000,0,-\n",
      "b,39.58,5,u,g,ff,ff,0,f,t,02,f,g,00017,1,-\n",
      "b,30.58,2.71,y,p,m,v,0.125,f,f,0,t,s,00080,0,-\n",
      "b,17.25,3,u,g,k,v,0.04,f,f,0,t,g,00160,40,-\n",
      "a,17.67,0,y,p,j,ff,0,f,f,0,f,g,00086,0,-\n",
      "b,16.50,0.125,u,g,c,v,0.165,f,f,0,f,g,00132,0,-\n",
      "a,27.33,1.665,u,g,ff,ff,0,f,f,0,f,g,00340,1,-\n",
      "b,31.25,1.125,u,g,ff,ff,0,f,t,01,f,g,00096,19,-\n",
      "b,20.00,7,u,g,c,v,0.5,f,f,0,f,g,00000,0,-\n",
      "b,39.50,1.625,u,g,c,v,1.5,f,f,0,f,g,00000,316,-\n",
      "b,36.50,4.25,u,g,q,v,3.5,f,f,0,f,g,00454,50,-\n",
      "b,52.42,1.5,u,g,d,v,3.75,f,f,0,t,g,00000,350,-\n",
      "b,36.17,18.125,u,g,w,v,0.085,f,f,0,f,g,00320,3552,-\n",
      "b,29.67,0.75,y,p,c,v,0.04,f,f,0,f,g,00240,0,-\n",
      "b,36.17,5.5,u,g,i,bb,5,f,f,0,f,g,00210,687,-\n",
      "b,25.67,0.29,y,p,c,v,1.5,f,f,0,t,g,00160,0,-\n",
      "a,24.50,2.415,y,p,c,v,0,f,f,0,f,g,00120,0,-\n",
      "b,24.08,0.875,u,g,m,v,0.085,f,t,04,f,g,00254,1950,-\n",
      "b,21.92,0.5,u,g,c,v,0.125,f,f,0,f,g,00360,0,-\n",
      "a,36.58,0.29,u,g,ff,ff,0,f,t,10,f,g,00200,18,-\n",
      "a,23.00,1.835,u,g,j,j,0,f,t,01,f,g,00200,53,-\n",
      "a,27.58,3,u,g,m,v,2.79,f,t,01,t,g,00280,10,-\n",
      "b,31.08,3.085,u,g,c,v,2.5,f,t,02,t,g,00160,41,-\n",
      "a,30.42,1.375,u,g,w,h,0.04,f,t,03,f,g,00000,33,-\n",
      "b,22.08,2.335,u,g,k,v,0.75,f,f,0,f,g,00180,0,-\n",
      "b,16.33,4.085,u,g,i,h,0.415,f,f,0,t,g,00120,0,-\n",
      "a,21.92,11.665,u,g,k,h,0.085,f,f,0,f,g,00320,5,-\n",
      "b,21.08,4.125,y,p,i,h,0.04,f,f,0,f,g,00140,100,-\n",
      "b,17.42,6.5,u,g,i,v,0.125,f,f,0,f,g,00060,100,-\n",
      "b,19.17,4,y,p,i,v,1,f,f,0,t,g,00360,1000,-\n",
      "b,20.67,0.415,u,g,c,v,0.125,f,f,0,f,g,00000,44,-\n",
      "b,26.75,2,u,g,d,v,0.75,f,f,0,t,g,00080,0,-\n",
      "b,23.58,0.835,u,g,i,h,0.085,f,f,0,t,g,00220,5,-\n",
      "b,39.17,2.5,y,p,i,h,10,f,f,0,t,s,00200,0,-\n",
      "b,22.75,11.5,u,g,i,v,0.415,f,f,0,f,g,00000,0,-\n",
      "a,16.92,0.5,u,g,i,v,0.165,f,t,06,t,g,00240,35,-\n",
      "b,23.50,3.165,y,p,k,v,0.415,f,t,01,t,g,00280,80,-\n",
      "a,17.33,9.5,u,g,aa,v,1.75,f,t,10,t,g,00000,10,-\n",
      "b,23.75,0.415,y,p,c,v,0.04,f,t,02,f,g,00128,6,-\n",
      "b,34.67,1.08,u,g,m,v,1.165,f,f,0,f,s,00028,0,-\n",
      "b,74.83,19,y,p,ff,ff,0.04,f,t,02,f,g,00000,351,-\n",
      "b,28.17,0.125,y,p,k,v,0.085,f,f,0,f,g,00216,2100,-\n",
      "b,24.50,13.335,y,p,aa,v,0.04,f,f,0,t,g,00120,475,-\n",
      "b,18.83,3.54,y,p,ff,ff,0,f,f,0,t,g,00180,1,-\n",
      "a,47.25,0.75,u,g,q,h,2.75,t,t,01,f,g,00333,892,+\n",
      "b,24.17,0.875,u,g,q,v,4.625,t,t,02,t,g,00520,2000,+\n",
      "b,39.25,9.5,u,g,m,v,6.5,t,t,14,f,g,00240,4607,+\n",
      "a,20.50,11.835,u,g,c,h,6,t,f,0,f,g,00340,0,+\n",
      "a,18.83,4.415,y,p,c,h,3,t,f,0,f,g,00240,0,+\n",
      "b,19.17,9.5,u,g,w,v,1.5,t,f,0,f,g,00120,2206,+\n",
      "a,25.00,0.875,u,g,x,h,1.04,t,f,0,t,g,00160,5860,+\n",
      "b,20.17,9.25,u,g,c,v,1.665,t,t,03,t,g,00040,28,+\n",
      "b,25.75,0.5,u,g,c,v,1.46,t,t,05,t,g,00312,0,+\n",
      "b,20.42,7,u,g,c,v,1.625,t,t,03,f,g,00200,1391,+\n",
      "b,39.00,5,u,g,cc,v,3.5,t,t,10,t,g,00000,0,+\n",
      "a,64.08,0.165,u,g,ff,ff,0,t,t,01,f,g,00232,100,+\n",
      "b,28.25,5.125,u,g,x,v,4.75,t,t,02,f,g,00420,7,+\n",
      "a,28.75,3.75,u,g,c,v,1.085,t,t,01,t,g,00371,0,+\n",
      "b,31.33,19.5,u,g,c,v,7,t,t,16,f,g,00000,5000,+\n",
      "a,18.92,9,u,g,aa,v,0.75,t,t,02,f,g,00088,591,+\n",
      "a,24.75,3,u,g,q,h,1.835,t,t,19,f,g,00000,500,+\n",
      "a,30.67,12,u,g,c,v,2,t,t,01,f,g,00220,19,+\n",
      "b,21.00,4.79,y,p,w,v,2.25,t,t,01,t,g,00080,300,+\n",
      "b,13.75,4,y,p,w,v,1.75,t,t,02,t,g,00120,1000,+\n",
      "a,46.00,4,u,g,j,j,0,t,f,0,f,g,00100,960,+\n",
      "a,44.33,0,u,g,c,v,2.5,t,f,0,f,g,00000,0,+\n",
      "b,20.25,9.96,u,g,e,dd,0,t,f,0,f,g,00000,0,+\n",
      "b,22.67,2.54,y,p,c,h,2.585,t,f,0,f,g,00000,0,+\n",
      "a,60.92,5,u,g,aa,v,4,t,t,04,f,g,00000,99,+\n",
      "b,16.08,0.75,u,g,c,v,1.75,t,t,05,t,g,00352,690,+\n",
      "a,28.17,0.375,u,g,q,v,0.585,t,t,04,f,g,00080,0,+\n",
      "b,39.17,1.71,u,g,x,v,0.125,t,t,05,t,g,00480,0,+\n",
      "a,30.00,5.29,u,g,e,dd,2.25,t,t,05,t,g,00099,500,+\n",
      "b,22.83,3,u,g,m,v,1.29,t,t,01,f,g,00260,800,+\n",
      "a,22.50,8.5,u,g,q,v,1.75,t,t,10,f,g,00080,990,-\n",
      "a,28.58,1.665,u,g,q,v,2.415,t,f,0,t,g,00440,0,-\n",
      "b,45.17,1.5,u,g,c,v,2.5,t,f,0,t,g,00140,0,-\n",
      "b,41.58,1.75,u,g,k,v,0.21,t,f,0,f,g,00160,0,-\n",
      "a,57.08,0.335,u,g,i,bb,1,t,f,0,t,g,00252,2197,-\n",
      "a,55.75,7.08,u,g,k,h,6.75,t,t,03,t,g,00100,50,-\n",
      "b,43.25,25.21,u,g,q,h,0.21,t,t,01,f,g,00760,90,-\n",
      "a,25.33,2.085,u,g,c,h,2.75,t,f,0,t,g,00360,1,-\n",
      "a,24.58,0.67,u,g,aa,h,1.75,t,f,0,f,g,00400,0,-\n",
      "b,43.17,2.25,u,g,i,bb,0.75,t,f,0,f,g,00560,0,-\n",
      "b,40.92,0.835,u,g,ff,ff,0,t,f,0,f,g,00130,1,-\n",
      "b,31.83,2.5,u,g,aa,v,7.5,t,f,0,t,g,00523,0,-\n",
      "a,33.92,1.585,y,p,ff,ff,0,t,f,0,f,g,00320,0,-\n",
      "a,24.92,1.25,u,g,ff,ff,0,t,f,0,f,g,00080,0,-\n",
      "b,35.25,3.165,u,g,x,h,3.75,t,f,0,t,g,00680,0,-\n",
      "b,34.25,1.75,u,g,w,bb,0.25,t,f,0,t,g,00163,0,-\n",
      "b,19.42,1.5,y,p,cc,v,2,t,f,0,t,g,00100,20,-\n",
      "b,42.75,3,u,g,i,bb,1,t,f,0,f,g,00000,200,-\n",
      "b,19.67,10,y,p,k,h,0.835,t,f,0,t,g,00140,0,-\n",
      "b,36.33,3.79,u,g,w,v,1.165,t,f,0,t,g,00200,0,-\n",
      "b,30.08,1.04,y,p,i,bb,0.5,t,t,10,t,g,00132,28,-\n",
      "b,44.25,11,y,p,d,v,1.5,t,f,0,f,s,00000,0,-\n",
      "b,23.58,0.46,y,p,w,v,2.625,t,t,06,t,g,00208,347,-\n",
      "b,23.92,1.5,u,g,d,h,1.875,t,t,06,f,g,00200,327,+\n",
      "b,33.17,1,u,g,x,v,0.75,t,t,07,t,g,00340,4071,+\n",
      "b,48.33,12,u,g,m,v,16,t,f,0,f,s,00110,0,+\n",
      "b,76.75,22.29,u,g,e,z,12.75,t,t,01,t,g,00000,109,+\n",
      "b,51.33,10,u,g,i,bb,0,t,t,11,f,g,00000,1249,+\n",
      "b,34.75,15,u,g,r,n,5.375,t,t,09,t,g,00000,134,+\n",
      "b,38.58,3.335,u,g,w,v,4,t,t,14,f,g,00383,1344,+\n",
      "a,22.42,11.25,y,p,x,h,0.75,t,t,04,f,g,00000,321,+\n",
      "b,41.92,0.42,u,g,c,h,0.21,t,t,06,f,g,00220,948,+\n",
      "b,29.58,4.5,u,g,w,v,7.5,t,t,02,t,g,00330,0,+\n",
      "a,32.17,1.46,u,g,w,v,1.085,t,t,16,f,g,00120,2079,+\n",
      "b,51.42,0.04,u,g,x,h,0.04,t,f,0,f,g,00000,3000,+\n",
      "a,22.83,2.29,u,g,q,h,2.29,t,t,07,t,g,00140,2384,+\n",
      "a,25.00,12.33,u,g,cc,h,3.5,t,t,06,f,g,00400,458,+\n",
      "b,26.75,1.125,u,g,x,h,1.25,t,f,0,f,g,00000,5298,+\n",
      "b,23.33,1.5,u,g,c,h,1.415,t,f,0,f,g,00422,200,+\n",
      "b,24.42,12.335,u,g,q,h,1.585,t,f,0,t,g,00120,0,+\n",
      "b,42.17,5.04,u,g,q,h,12.75,t,f,0,t,g,00092,0,+\n",
      "a,20.83,3,u,g,aa,v,0.04,t,f,0,f,g,00100,0,+\n",
      "b,23.08,11.5,u,g,w,h,2.125,t,t,11,t,g,00290,284,+\n",
      "a,25.17,2.875,u,g,x,h,0.875,t,f,0,f,g,00360,0,+\n",
      "b,43.08,0.375,y,p,c,v,0.375,t,t,08,t,g,00300,162,+\n",
      "a,35.75,0.915,u,g,aa,v,0.75,t,t,04,f,g,00000,1583,+\n",
      "b,59.50,2.75,u,g,w,v,1.75,t,t,05,t,g,00060,58,+\n",
      "b,21.00,3,y,p,d,v,1.085,t,t,08,t,g,00160,1,+\n",
      "b,21.92,0.54,y,p,x,v,0.04,t,t,01,t,g,00840,59,+\n",
      "a,65.17,14,u,g,ff,ff,0,t,t,11,t,g,00000,1400,+\n",
      "a,20.33,10,u,g,c,h,1,t,t,04,f,g,00050,1465,+\n",
      "b,32.25,0.165,y,p,c,h,3.25,t,t,01,t,g,00432,8000,+\n",
      "b,30.17,0.5,u,g,c,v,1.75,t,t,11,f,g,00032,540,+\n",
      "b,25.17,6,u,g,c,v,1,t,t,03,f,g,00000,0,+\n",
      "b,39.17,1.625,u,g,c,v,1.5,t,t,10,f,g,00186,4700,+\n",
      "b,39.08,6,u,g,m,v,1.29,t,t,05,t,g,00108,1097,+\n",
      "b,31.67,0.83,u,g,x,v,1.335,t,t,08,t,g,00303,3290,+\n",
      "b,41.00,0.04,u,g,e,v,0.04,f,t,01,f,s,00560,0,+\n",
      "b,48.50,4.25,u,g,m,v,0.125,t,f,0,t,g,00225,0,+\n",
      "b,32.67,9,y,p,w,h,5.25,t,f,0,t,g,00154,0,+\n",
      "a,28.08,15,y,p,e,z,0,t,f,0,f,g,00000,13212,+\n",
      "b,73.42,17.75,u,g,ff,ff,0,t,f,0,t,g,00000,0,+\n",
      "b,64.08,20,u,g,x,h,17.5,t,t,09,t,g,00000,1000,+\n",
      "b,51.58,15,u,g,c,v,8.5,t,t,09,f,g,00000,0,+\n",
      "b,26.67,1.75,y,p,c,v,1,t,t,05,t,g,00160,5777,+\n",
      "b,25.33,0.58,u,g,c,v,0.29,t,t,07,t,g,00096,5124,+\n",
      "b,30.17,6.5,u,g,cc,v,3.125,t,t,08,f,g,00330,1200,+\n",
      "b,27.00,0.75,u,g,c,h,4.25,t,t,03,t,g,00312,150,+\n",
      "b,34.17,5.25,u,g,w,v,0.085,f,f,0,t,g,00290,6,+\n",
      "b,38.67,0.21,u,g,k,v,0.085,t,f,0,t,g,00280,0,+\n",
      "b,25.75,0.75,u,g,c,bb,0.25,t,f,0,f,g,00349,23,+\n",
      "a,46.08,3,u,g,c,v,2.375,t,t,08,t,g,00396,4159,+\n",
      "a,21.50,6,u,g,aa,v,2.5,t,t,03,f,g,00080,918,+\n",
      "b,20.50,2.415,u,g,c,v,2,t,t,11,t,g,00200,3000,+\n",
      "a,29.50,0.46,u,g,k,v,0.54,t,t,04,f,g,00380,500,+\n",
      "b,29.83,1.25,y,p,k,v,0.25,f,f,0,f,g,00224,0,-\n",
      "b,20.08,0.25,u,g,q,v,0.125,f,f,0,f,g,00200,0,-\n",
      "b,23.42,0.585,u,g,c,h,0.085,t,f,0,f,g,00180,0,-\n",
      "a,29.58,1.75,y,p,k,v,1.25,f,f,0,t,g,00280,0,-\n",
      "b,16.17,0.04,u,g,c,v,0.04,f,f,0,f,g,00000,0,+\n",
      "b,32.33,3.5,u,g,k,v,0.5,f,f,0,t,g,00232,0,-\n",
      "b,47.83,4.165,u,g,x,bb,0.085,f,f,0,t,g,00520,0,-\n",
      "b,20.00,1.25,y,p,k,v,0.125,f,f,0,f,g,00140,4,-\n",
      "b,27.58,3.25,y,p,q,h,5.085,f,t,02,t,g,00369,1,-\n",
      "b,22.00,0.79,u,g,w,v,0.29,f,t,01,f,g,00420,283,-\n",
      "b,19.33,10.915,u,g,c,bb,0.585,f,t,02,t,g,00200,7,-\n",
      "a,38.33,4.415,u,g,c,v,0.125,f,f,0,f,g,00160,0,-\n",
      "b,29.42,1.25,u,g,c,h,0.25,f,t,02,t,g,00400,108,-\n",
      "b,22.67,0.75,u,g,i,v,1.585,f,t,01,t,g,00400,9,-\n",
      "b,32.25,14,y,p,ff,ff,0,f,t,02,f,g,00160,1,-\n",
      "b,29.58,4.75,u,g,m,v,2,f,t,01,t,g,00460,68,-\n",
      "b,18.42,10.415,y,p,aa,v,0.125,t,f,0,f,g,00120,375,-\n",
      "b,22.17,2.25,u,g,i,v,0.125,f,f,0,f,g,00160,10,-\n",
      "b,22.67,0.165,u,g,c,j,2.25,f,f,0,t,s,00000,0,+\n",
      "b,18.83,0,u,g,q,v,0.665,f,f,0,f,g,00160,1,-\n",
      "b,21.58,0.79,y,p,cc,v,0.665,f,f,0,f,g,00160,0,-\n",
      "b,23.75,12,u,g,c,v,2.085,f,f,0,f,s,00080,0,-\n",
      "b,36.08,2.54,u,g,ff,ff,0,f,f,0,f,g,00000,1000,-\n",
      "b,29.25,13,u,g,d,h,0.5,f,f,0,f,g,00228,0,-\n",
      "a,19.58,0.665,u,g,w,v,1.665,f,f,0,f,g,00220,5,-\n",
      "a,22.92,1.25,u,g,q,v,0.25,f,f,0,t,g,00120,809,-\n",
      "a,27.25,0.29,u,g,m,h,0.125,f,t,01,t,g,00272,108,-\n",
      "a,38.75,1.5,u,g,ff,ff,0,f,f,0,f,g,00076,0,-\n",
      "b,32.42,2.165,y,p,k,ff,0,f,f,0,f,g,00120,0,-\n",
      "a,23.75,0.71,u,g,w,v,0.25,f,t,01,t,g,00240,4,-\n",
      "b,18.17,2.46,u,g,c,n,0.96,f,t,02,t,g,00160,587,-\n",
      "b,40.92,0.5,y,p,m,v,0.5,f,f,0,t,g,00130,0,-\n",
      "b,19.50,9.585,u,g,aa,v,0.79,f,f,0,f,g,00080,350,-\n",
      "b,28.58,3.625,u,g,aa,v,0.25,f,f,0,t,g,00100,0,-\n",
      "b,35.58,0.75,u,g,k,v,1.5,f,f,0,t,g,00231,0,-\n",
      "b,34.17,2.75,u,g,i,bb,2.5,f,f,0,t,g,00232,200,-\n",
      "b,31.58,0.75,y,p,aa,v,3.5,f,f,0,t,g,00320,0,-\n",
      "a,52.50,7,u,g,aa,h,3,f,f,0,f,g,00000,0,-\n",
      "b,36.17,0.42,y,p,w,v,0.29,f,f,0,t,g,00309,2,-\n",
      "b,37.33,2.665,u,g,cc,v,0.165,f,f,0,t,g,00000,501,-\n",
      "a,20.83,8.5,u,g,c,v,0.165,f,f,0,f,g,00000,351,-\n",
      "b,24.08,9,u,g,aa,v,0.25,f,f,0,t,g,00000,0,-\n",
      "b,25.58,0.335,u,g,k,h,3.5,f,f,0,t,g,00340,0,-\n",
      "a,35.17,3.75,u,g,ff,ff,0,f,t,06,f,g,00000,200,-\n",
      "b,48.08,3.75,u,g,i,bb,1,f,f,0,f,g,00100,2,-\n",
      "a,15.83,7.625,u,g,q,v,0.125,f,t,01,t,g,00000,160,-\n",
      "a,22.50,0.415,u,g,i,v,0.335,f,f,0,t,s,00144,0,-\n",
      "b,21.50,11.5,u,g,i,v,0.5,t,f,0,t,g,00100,68,-\n",
      "a,23.58,0.83,u,g,q,v,0.415,f,t,01,t,g,00200,11,-\n",
      "a,21.08,5,y,p,ff,ff,0,f,f,0,f,g,00000,0,-\n",
      "b,25.67,3.25,u,g,c,h,2.29,f,t,01,t,g,00416,21,-\n",
      "a,38.92,1.665,u,g,aa,v,0.25,f,f,0,f,g,00000,390,-\n",
      "a,15.75,0.375,u,g,c,v,1,f,f,0,f,g,00120,18,-\n",
      "a,28.58,3.75,u,g,c,v,0.25,f,t,01,t,g,00040,154,-\n",
      "b,22.25,9,u,g,aa,v,0.085,f,f,0,f,g,00000,0,-\n",
      "b,29.83,3.5,u,g,c,v,0.165,f,f,0,f,g,00216,0,-\n",
      "a,23.50,1.5,u,g,w,v,0.875,f,f,0,t,g,00160,0,-\n",
      "b,32.08,4,y,p,cc,v,1.5,f,f,0,t,g,00120,0,-\n",
      "b,31.08,1.5,y,p,w,v,0.04,f,f,0,f,s,00160,0,-\n",
      "b,31.83,0.04,y,p,m,v,0.04,f,f,0,f,g,00000,0,-\n",
      "a,21.75,11.75,u,g,c,v,0.25,f,f,0,t,g,00180,0,-\n",
      "a,17.92,0.54,u,g,c,v,1.75,f,t,01,t,g,00080,5,-\n",
      "b,30.33,0.5,u,g,d,h,0.085,f,f,0,t,s,00252,0,-\n",
      "b,51.83,2.04,y,p,ff,ff,1.5,f,f,0,f,g,00120,1,-\n",
      "b,47.17,5.835,u,g,w,v,5.5,f,f,0,f,g,00465,150,-\n",
      "b,25.83,12.835,u,g,cc,v,0.5,f,f,0,f,g,00000,2,-\n",
      "a,50.25,0.835,u,g,aa,v,0.5,f,f,0,t,g,00240,117,-\n",
      "a,37.33,2.5,u,g,i,h,0.21,f,f,0,f,g,00260,246,-\n",
      "a,41.58,1.04,u,g,aa,v,0.665,f,f,0,f,g,00240,237,-\n",
      "a,30.58,10.665,u,g,q,h,0.085,f,t,12,t,g,00129,3,-\n",
      "b,19.42,7.25,u,g,m,v,0.04,f,t,01,f,g,00100,1,-\n",
      "a,17.92,10.21,u,g,ff,ff,0,f,f,0,f,g,00000,50,-\n",
      "a,20.08,1.25,u,g,c,v,0,f,f,0,f,g,00000,0,-\n",
      "b,19.50,0.29,u,g,k,v,0.29,f,f,0,f,g,00280,364,-\n",
      "b,27.83,1,y,p,d,h,3,f,f,0,f,g,00176,537,-\n",
      "b,17.08,3.29,u,g,i,v,0.335,f,f,0,t,g,00140,2,-\n",
      "b,36.42,0.75,y,p,d,v,0.585,f,f,0,f,g,00240,3,-\n",
      "b,40.58,3.29,u,g,m,v,3.5,f,f,0,t,s,00400,0,-\n",
      "b,21.08,10.085,y,p,e,h,1.25,f,f,0,f,g,00260,0,-\n",
      "a,22.67,0.75,u,g,c,v,2,f,t,02,t,g,00200,394,-\n",
      "a,25.25,13.5,y,p,ff,ff,2,f,t,01,t,g,00200,1,-\n",
      "b,17.92,0.205,u,g,aa,v,0.04,f,f,0,f,g,00280,750,-\n",
      "b,35.00,3.375,u,g,c,h,8.29,f,f,0,t,g,00000,0,-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove missing values\n",
    "cleaned_data = \"\"\n",
    "cleaned_p_count = 0\n",
    "cleaned_n_count = 0\n",
    "\n",
    "with open('Dataset/data.csv', 'r') as f:\n",
    "    data = f.readlines()\n",
    "    for i, row in enumerate(data):\n",
    "        # Check for '?' value in each row (indicates missing)\n",
    "        if '?' not in row:\n",
    "            cleaned_data += row\n",
    "            if '+' in row:\n",
    "                cleaned_p_count += 1\n",
    "            elif '-' in row:\n",
    "                cleaned_n_count += 1\n",
    "\n",
    "    print(cleaned_data)\n",
    "\n",
    "with open('./dataset/crx_clean.data.txt', 'w') as f:\n",
    "    f.write(cleaned_data)\n",
    "\n",
    "with open('./dataset/crx_clean.names.txt', 'w') as f:\n",
    "    f.write(\"Class Distribution\\n\")\n",
    "    f.write(\"+ Classes: %d\\n\" %cleaned_p_count)\n",
    "    f.write(\"- Classes: %d\\n\" %cleaned_n_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_category(credit_data):\n",
    "\t\"\"\"\n",
    "\tSplits 'category' columns into one-hot columns\n",
    "\targ, return\n",
    "\t\tcredit_data: Dataframe\n",
    "\t\"\"\"\n",
    "\tcat_columns = []\n",
    "\tfor i, _ in enumerate(credit_data):\n",
    "\t\t# dtype == 'object' after ensuring data has been cleaned\n",
    "\t\t# i.e no 'float' dtypes as 'object' because of '?' values\n",
    "\t\tif credit_data[i].dtype == 'object' and not i==15:\n",
    "\t\t\tcat_columns.append(i)\n",
    "\n",
    "\n",
    "\t# get_dummies() one-hot encodes data\n",
    "\tcredit_data = pd.get_dummies(credit_data, columns=cat_columns)\n",
    "\t\n",
    "\treturn credit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 100\n",
    "\n",
    "def import_data(url):\n",
    "\t\"\"\"\n",
    "\targs\n",
    "\t\turl: url string of CLEANED csv data\n",
    "\treturns\n",
    "\t\tcredit_data: Dataframe\n",
    "\t\"\"\"\n",
    "\n",
    "\tcredit_data = pd.read_csv(url, sep=',', header=None)\n",
    "\n",
    "\t# Bring class attribute to first column\n",
    "\tcols = credit_data.columns.tolist()\n",
    "\tcols = cols[-1:] + cols[:-1]\n",
    "\tcredit_data = credit_data[cols]\n",
    "\tprint(\"Reordered Dataset: \\n\", credit_data.head())\n",
    "\n",
    "\tcredit_data = one_hot_encode_category(credit_data)\n",
    "\tprint(\"Dataset length: \", len(credit_data))\n",
    "\tprint(\"Dataset shape: \", credit_data.shape)\n",
    "\tprint(\"One-hot Dataset: \\n\", credit_data.head())\n",
    "\t# print(credit_data.info())\n",
    "\treturn credit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reordered Dataset: \n",
      "   15 0      1      2  3  4  5  6     7  8  9   10 11 12   13   14\n",
      "0  +  b  30.83  0.000  u  g  w  v  1.25  t  t   1  f  g  202    0\n",
      "1  +  a  58.67  4.460  u  g  q  h  3.04  t  t   6  f  g   43  560\n",
      "2  +  a  24.50  0.500  u  g  q  h  1.50  t  f   0  f  g  280  824\n",
      "3  +  b  27.83  1.540  u  g  w  v  3.75  t  t   5  t  g  100    3\n",
      "4  +  b  20.17  5.625  u  g  w  v  1.71  t  f   0  f  s  120    0\n",
      "Dataset length:  653\n",
      "Dataset shape:  (653, 47)\n",
      "One-hot Dataset: \n",
      "   15      1      2     7  10   13   14  0_a  0_b  3_l  ...  6_z  8_f  8_t  \\\n",
      "0  +  30.83  0.000  1.25   1  202    0    0    1    0  ...    0    0    1   \n",
      "1  +  58.67  4.460  3.04   6   43  560    1    0    0  ...    0    0    1   \n",
      "2  +  24.50  0.500  1.50   0  280  824    1    0    0  ...    0    0    1   \n",
      "3  +  27.83  1.540  3.75   5  100    3    0    1    0  ...    0    0    1   \n",
      "4  +  20.17  5.625  1.71   0  120    0    0    1    0  ...    0    0    1   \n",
      "\n",
      "   9_f  9_t  11_f  11_t  12_g  12_p  12_s  \n",
      "0    0    1     1     0     1     0     0  \n",
      "1    0    1     1     0     1     0     0  \n",
      "2    1    0     1     0     1     0     0  \n",
      "3    0    1     0     1     1     0     0  \n",
      "4    1    0     1     0     0     0     1  \n",
      "\n",
      "[5 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "#Read filtered data and feature setup and split to feed\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Building Phase\n",
    "data = import_data(\"dataset/crx_clean.data.txt\")\n",
    "\n",
    "X = data.values[:, 1:]\n",
    "Y = data.values[:, 0]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = seed)\n",
    "\n",
    "#Float conversion\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "\n",
    "Y_train = np.where(Y_train=='+', 1, Y_train)\n",
    "Y_train = np.where(Y_train=='-', 0, Y_train)\n",
    "Y_train = Y_train.astype(np.float)\n",
    "\n",
    "Y_test = np.where(Y_test=='+', 1, Y_test)\n",
    "Y_test = np.where(Y_test=='-', 0, Y_test)\n",
    "Y_test = Y_test.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge inputs and targets\n",
    "inputs = np.concatenate((X_train, X_test), axis=0)\n",
    "targets = np.concatenate((Y_train, Y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(optimizerF, lossF, activationF):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(46, input_dim=46, activation=activationF))\n",
    "    model.add(Dense(20, activation=activationF))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=lossF,optimizer=optimizerF,metrics=[f1_m])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(optimizerF, lossF, activationF):\n",
    "    print(f'************************* {optimizerF} --- {lossF} --- {activationF}****************************')\n",
    "    # Define per-fold score containers\n",
    "    f1_per_fold = []\n",
    "    loss_per_fold = []\n",
    "\n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(inputs, targets):\n",
    "\n",
    "    # Define the model architecture\n",
    "        model = get_model(optimizerF, lossF, activationF)\n",
    "#         print('-------------------------------------------------------------------------------------------------')\n",
    "#         print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "        history = model.fit(inputs[train], targets[train],batch_size=10,epochs=50, verbose=0)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "        scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "#         print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]} {model.metrics_names[1]} of {scores[1]}')\n",
    "        f1_per_fold.append(scores[1])\n",
    "        loss_per_fold.append(scores[0])\n",
    "    # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "    # == Provide average scores ==\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Score per fold')\n",
    "    for i in range(0, len(f1_per_fold)):\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - F1 score: {f1_per_fold[i]}')\n",
    "\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('F1 scores for all folds:')\n",
    "    print(f'> F1: {np.mean(f1_per_fold)} (+- {np.std(f1_per_fold)})')\n",
    "    print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "    print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimzers = ['adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']\n",
    "lossFunctions = ['binary_crossentropy', 'mean_squared_error']\n",
    "activationFunctions = ['relu', 'tanh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************* adam --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 1.2762517929077148 - F1 score: 0.8335363268852234\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 1.1187858581542969 - F1 score: 0.8314062356948853\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 1.9337730407714844 - F1 score: 0.7446077466011047\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.8145887851715088 - F1 score: 0.753416121006012\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 5.829875469207764 - F1 score: 0.5926430821418762\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7511219024658203 (+- 0.08763668828866819)\n",
      "> Loss: 2.1946549892425535\n",
      "------------------------------------------------------------------------\n",
      "************************* adam --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.40832453966140747 - F1 score: 0.7326871156692505\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5242252945899963 - F1 score: 0.8141983151435852\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.49175721406936646 - F1 score: 0.8067445755004883\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.4791063964366913 - F1 score: 0.8032343983650208\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5666136145591736 - F1 score: 0.5921211838722229\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7497971177101135 (+- 0.08414335674898171)\n",
      "> Loss: 0.494005411863327\n",
      "------------------------------------------------------------------------\n",
      "************************* adam --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.23111596703529358 - F1 score: 0.7416085004806519\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.1842149794101715 - F1 score: 0.7661205530166626\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.26912403106689453 - F1 score: 0.7793649435043335\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.18693770468235016 - F1 score: 0.7209762334823608\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.17587080597877502 - F1 score: 0.6237404942512512\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.726362144947052 (+- 0.05510479154851565)\n",
      "> Loss: 0.20945269763469695\n",
      "------------------------------------------------------------------------\n",
      "************************* adam --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.16441336274147034 - F1 score: 0.7410191297531128\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.1301269382238388 - F1 score: 0.8265508413314819\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.12585164606571198 - F1 score: 0.8179448246955872\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.13690191507339478 - F1 score: 0.6531714200973511\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.18041186034679413 - F1 score: 0.580552875995636\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7238478183746337 (+- 0.09510584129520804)\n",
      "> Loss: 0.147541144490242\n",
      "------------------------------------------------------------------------\n",
      "************************* SGD --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.8667367696762085 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6651472449302673 - F1 score: 0.1813025027513504\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6995136737823486 - F1 score: 0.15475322306156158\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6823781728744507 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6862038969993591 - F1 score: 0.2722221910953522\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.12165558338165283 (+- 0.10669927275942219)\n",
      "> Loss: 0.7199959516525268\n",
      "------------------------------------------------------------------------\n",
      "************************* SGD --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6526495814323425 - F1 score: 0.6188843846321106\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6159692406654358 - F1 score: 0.7295158505439758\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5509147047996521 - F1 score: 0.5454545021057129\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.594469428062439 - F1 score: 0.522377610206604\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6457911133766174 - F1 score: 0.46644505858421326\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5765354812145234 (+- 0.09079143839618453)\n",
      "> Loss: 0.6119588136672973\n",
      "------------------------------------------------------------------------\n",
      "************************* SGD --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.3464907705783844 - F1 score: 0.5828163027763367\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.1849406510591507 - F1 score: 0.7248442769050598\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.2955743372440338 - F1 score: 0.5094732046127319\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.31614506244659424 - F1 score: 0.49069589376449585\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.44685792922973633 - F1 score: 0.0673912987112999\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.47504419535398484 (+- 0.21982999084020483)\n",
      "> Loss: 0.3180017501115799\n",
      "------------------------------------------------------------------------\n",
      "************************* SGD --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.22966928780078888 - F1 score: 0.5829614400863647\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.1951998621225357 - F1 score: 0.731467068195343\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.2308564931154251 - F1 score: 0.46630239486694336\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.20463459193706512 - F1 score: 0.4457048773765564\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.20069639384746552 - F1 score: 0.5282050967216492\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5509281754493713 (+- 0.10234278219846975)\n",
      "> Loss: 0.21221132576465607\n",
      "------------------------------------------------------------------------\n",
      "************************* RMSprop --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.9235879778862 - F1 score: 0.7989276647567749\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.4518962502479553 - F1 score: 0.7715950012207031\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.7289080023765564 - F1 score: 0.7809638381004333\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 1.1631057262420654 - F1 score: 0.8152531385421753\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.8029183745384216 - F1 score: 0.7624000310897827\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7858279347419739 (+- 0.019039685120503316)\n",
      "> Loss: 0.8140832662582398\n",
      "------------------------------------------------------------------------\n",
      "************************* RMSprop --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.43923714756965637 - F1 score: 0.8353586196899414\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5608731508255005 - F1 score: 0.7798561453819275\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5505316257476807 - F1 score: 0.6095976829528809\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.3452202379703522 - F1 score: 0.684118390083313\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.43646231293678284 - F1 score: 0.8151701092720032\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7448201894760131 (+- 0.08525687243507814)\n",
      "> Loss: 0.4664648950099945\n",
      "------------------------------------------------------------------------\n",
      "************************* RMSprop --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.19700302183628082 - F1 score: 0.7996729016304016\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.16043469309806824 - F1 score: 0.8760125041007996\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.18720771372318268 - F1 score: 0.7394181489944458\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.16814987361431122 - F1 score: 0.8280951380729675\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2892146706581116 - F1 score: 0.5709773302078247\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7628352046012878 (+- 0.10562459791280705)\n",
      "> Loss: 0.2004019945859909\n",
      "------------------------------------------------------------------------\n",
      "************************* RMSprop --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.11618834733963013 - F1 score: 0.7885926961898804\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.13866297900676727 - F1 score: 0.8266976475715637\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.13102515041828156 - F1 score: 0.8290411829948425\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.14303547143936157 - F1 score: 0.5690475702285767\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.13609528541564941 - F1 score: 0.7425300478935242\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7511818289756775 (+- 0.09634524625268916)\n",
      "> Loss: 0.133001446723938\n",
      "------------------------------------------------------------------------\n",
      "************************* Adadelta --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 1.5350284576416016 - F1 score: 0.6726774573326111\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 4.984096050262451 - F1 score: 0.6637842059135437\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 87.89959716796875 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 7.049717903137207 - F1 score: 0.4563814103603363\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 51.694759368896484 - F1 score: 0.10493505001068115\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.37955562472343446 (+- 0.2800312228202389)\n",
      "> Loss: 30.632639789581297\n",
      "------------------------------------------------------------------------\n",
      "************************* Adadelta --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.7857127785682678 - F1 score: 0.2567557394504547\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6366502642631531 - F1 score: 0.7356268167495728\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.733723521232605 - F1 score: 0.3060987889766693\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.8825912475585938 - F1 score: 0.514083743095398\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.8349189162254333 - F1 score: 0.6030493974685669\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.48312289714813234 (+- 0.17982146358707865)\n",
      "> Loss: 0.7747193455696106\n",
      "------------------------------------------------------------------------\n",
      "************************* Adadelta --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.24322837591171265 - F1 score: 0.64897620677948\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5028864145278931 - F1 score: 0.6103915572166443\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.4884724020957947 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.27702727913856506 - F1 score: 0.5163058638572693\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6283669471740723 - F1 score: 0.3944135308265686\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.43401743173599244 (+- 0.23410514440575186)\n",
      "> Loss: 0.4279962837696075\n",
      "------------------------------------------------------------------------\n",
      "************************* Adadelta --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.34943586587905884 - F1 score: 0.21008360385894775\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.3048825263977051 - F1 score: 0.49393194913864136\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.2775646150112152 - F1 score: 0.5435353517532349\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.21812383830547333 - F1 score: 0.48227229714393616\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2602192461490631 - F1 score: 0.6075726747512817\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.46747917532920835 (+- 0.13607482975209306)\n",
      "> Loss: 0.2820452183485031\n",
      "------------------------------------------------------------------------\n",
      "************************* Adagrad --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5568395256996155 - F1 score: 0.5066240429878235\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6956403255462646 - F1 score: 0.4484330117702484\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5333026647567749 - F1 score: 0.7498832941055298\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.7714678049087524 - F1 score: 0.46050915122032166\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.7365953922271729 - F1 score: 0.5243925452232361\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5379684090614318 (+- 0.10962774818461656)\n",
      "> Loss: 0.658769142627716\n",
      "------------------------------------------------------------------------\n",
      "************************* Adagrad --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6097898483276367 - F1 score: 0.5846461057662964\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6422997713088989 - F1 score: 0.556062638759613\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6366645693778992 - F1 score: 0.5184502005577087\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.5583832859992981 - F1 score: 0.6286334991455078\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5870168209075928 - F1 score: 0.4980391561985016\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5571663200855255 (+- 0.04660230431413671)\n",
      "> Loss: 0.6068308591842652\n",
      "------------------------------------------------------------------------\n",
      "************************* Adagrad --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.272570937871933 - F1 score: 0.5985184907913208\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.33147597312927246 - F1 score: 0.21852938830852509\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.4176172614097595 - F1 score: 0.20451125502586365\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.43809083104133606 - F1 score: 0.214074045419693\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.21037732064723969 - F1 score: 0.582857072353363\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.3636980503797531 (+- 0.18545783093220486)\n",
      "> Loss: 0.33402646481990816\n",
      "------------------------------------------------------------------------\n",
      "************************* Adagrad --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.22013267874717712 - F1 score: 0.428154855966568\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.20992253720760345 - F1 score: 0.49843376874923706\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.22449401021003723 - F1 score: 0.504483699798584\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.19195696711540222 - F1 score: 0.6401968002319336\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.22039462625980377 - F1 score: 0.613602876663208\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5369744002819061 (+- 0.07862506771481335)\n",
      "> Loss: 0.21338016390800477\n",
      "------------------------------------------------------------------------\n",
      "************************* Adamax --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6403516530990601 - F1 score: 0.8135401606559753\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.4083620309829712 - F1 score: 0.8411110043525696\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.4548034071922302 - F1 score: 0.7688669562339783\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 1.1434645652770996 - F1 score: 0.6097401976585388\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.38396143913269043 - F1 score: 0.8792592287063599\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7825035095214844 (+- 0.09358728425423897)\n",
      "> Loss: 0.6061886191368103\n",
      "------------------------------------------------------------------------\n",
      "************************* Adamax --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5174335837364197 - F1 score: 0.7809182405471802\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.46996009349823 - F1 score: 0.8138351440429688\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.47089454531669617 - F1 score: 0.7467163801193237\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.3824397325515747 - F1 score: 0.8560736775398254\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5246492028236389 - F1 score: 0.738597571849823\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7872282028198242 (+- 0.04357428228121173)\n",
      "> Loss: 0.4730754315853119\n",
      "------------------------------------------------------------------------\n",
      "************************* Adamax --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.145091712474823 - F1 score: 0.5965240597724915\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2582605183124542 - F1 score: 0.6761995553970337\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.11068542301654816 - F1 score: 0.8169053196907043\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.1404588520526886 - F1 score: 0.8465034365653992\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.16445611417293549 - F1 score: 0.6145274639129639\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7101319670677185 (+- 0.10314660702957373)\n",
      "> Loss: 0.1637905240058899\n",
      "------------------------------------------------------------------------\n",
      "************************* Adamax --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.19383396208286285 - F1 score: 0.5635759234428406\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.16723962128162384 - F1 score: 0.749629557132721\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.1501900851726532 - F1 score: 0.7983886003494263\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.1415833979845047 - F1 score: 0.7400714159011841\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.1554947942495346 - F1 score: 0.5810946822166443\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6865520358085633 (+- 0.09549334275162692)\n",
      "> Loss: 0.16166837215423585\n",
      "------------------------------------------------------------------------\n",
      "************************* Nadam --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.7554242610931396 - F1 score: 0.8047183156013489\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5285224914550781 - F1 score: 0.6406698226928711\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.49810296297073364 - F1 score: 0.8644344210624695\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 10.715478897094727 - F1 score: 0.73643958568573\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5364400744438171 - F1 score: 0.6794414520263672\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7451407194137574 (+- 0.08134709453398142)\n",
      "> Loss: 2.606793737411499\n",
      "------------------------------------------------------------------------\n",
      "************************* Nadam --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5267255306243896 - F1 score: 0.8283818364143372\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5018134713172913 - F1 score: 0.6042236089706421\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5933547616004944 - F1 score: 0.7739993333816528\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.4149414896965027 - F1 score: 0.6072462797164917\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.510317862033844 - F1 score: 0.5159124135971069\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6659526944160461 (+- 0.11647000587884705)\n",
      "> Loss: 0.5094306230545044\n",
      "------------------------------------------------------------------------\n",
      "************************* Nadam --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.12818332016468048 - F1 score: 0.7979692220687866\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2449621558189392 - F1 score: 0.7516295909881592\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.38762298226356506 - F1 score: 0.3215857446193695\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.1716422438621521 - F1 score: 0.6264134049415588\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.3141508996486664 - F1 score: 0.3276190161705017\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5650433957576751 (+- 0.20419455955566862)\n",
      "> Loss: 0.24931232035160064\n",
      "------------------------------------------------------------------------\n",
      "************************* Nadam --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.1287669688463211 - F1 score: 0.8633699417114258\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.1389450877904892 - F1 score: 0.8316526412963867\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.1114620491862297 - F1 score: 0.6654335260391235\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.17305870354175568 - F1 score: 0.5695237517356873\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.11396262049674988 - F1 score: 0.840512752532959\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7540985226631165 (+- 0.11606173456489284)\n",
      "> Loss: 0.1332390859723091\n",
      "------------------------------------------------------------------------\n",
      "************************* Ftrl --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.47409483790397644 - F1 score: 0.7703624963760376\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5687988996505737 - F1 score: 0.5439451932907104\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5112677216529846 - F1 score: 0.5686609148979187\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6401621699333191 - F1 score: 0.7339779138565063\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.641396701335907 - F1 score: 0.513939380645752\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.626177179813385 (+- 0.10495517869242421)\n",
      "> Loss: 0.5671440660953522\n",
      "------------------------------------------------------------------------\n",
      "************************* Ftrl --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6269682049751282 - F1 score: 0.6448888182640076\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6100967526435852 - F1 score: 0.5231074690818787\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5963380336761475 - F1 score: 0.6666666269302368\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6388988494873047 - F1 score: 0.38644686341285706\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6006243824958801 - F1 score: 0.5298535227775574\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5501926600933075 (+- 0.1004923094892443)\n",
      "> Loss: 0.6145852446556092\n",
      "------------------------------------------------------------------------\n",
      "************************* Ftrl --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.24932974576950073 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.24151906371116638 - F1 score: 0.6944965720176697\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.24334244430065155 - F1 score: 0.4674246907234192\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.2218952178955078 - F1 score: 0.6367064118385315\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2135256826877594 - F1 score: 0.43490391969680786\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.44670631885528567 (+- 0.2439734115677046)\n",
      "> Loss: 0.23392243087291717\n",
      "------------------------------------------------------------------------\n",
      "************************* Ftrl --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.22884432971477509 - F1 score: 0.4848563075065613\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2502434551715851 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.21924932301044464 - F1 score: 0.5324115753173828\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.2484765499830246 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.22397641837596893 - F1 score: 0.7036189436912537\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.34417736530303955 (+- 0.2902884938978189)\n",
      "> Loss: 0.23415801525115967\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for opti in optimzers:\n",
    "    for los in lossFunctions:\n",
    "        for act in activationFunctions:\n",
    "            evaluateModel(opti,los,act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model(optimizerF, lossF, activationF):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(46, input_dim=46, activation=activationF))\n",
    "    model.add(Dense(10, activation=activationF))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=lossF,optimizer=optimizerF,metrics=[f1_m])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************* adam --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.7526336908340454 - F1 score: 0.8317810297012329\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.37800106406211853 - F1 score: 0.6457648277282715\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 1.48969566822052 - F1 score: 0.6592065095901489\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.46871066093444824 - F1 score: 0.8255302309989929\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 5.904057502746582 - F1 score: 0.7098664045333862\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7344298005104065 (+- 0.07987481652864678)\n",
      "> Loss: 1.7986197173595428\n",
      "------------------------------------------------------------------------\n",
      "************************* adam --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.40965425968170166 - F1 score: 0.620952308177948\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.47436070442199707 - F1 score: 0.8233535885810852\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.37336981296539307 - F1 score: 0.7952123284339905\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.5333147048950195 - F1 score: 0.5914813876152039\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.521425724029541 - F1 score: 0.8189464807510376\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7299892187118531 (+- 0.10193902217138402)\n",
      "> Loss: 0.46242504119873046\n",
      "------------------------------------------------------------------------\n",
      "************************* adam --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.22444359958171844 - F1 score: 0.810136616230011\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.33392956852912903 - F1 score: 0.5261384844779968\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.1627253293991089 - F1 score: 0.7587336301803589\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.2765055000782013 - F1 score: 0.7430811524391174\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.3288799524307251 - F1 score: 0.30281636118888855\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6281812489032745 (+- 0.18954086826730457)\n",
      "> Loss: 0.26529679000377654\n",
      "------------------------------------------------------------------------\n",
      "************************* adam --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.135980024933815 - F1 score: 0.840478777885437\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.16599446535110474 - F1 score: 0.7612314820289612\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.12400630116462708 - F1 score: 0.7905830144882202\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.11520759761333466 - F1 score: 0.6477551460266113\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.15079063177108765 - F1 score: 0.8188669085502625\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7717830657958984 (+- 0.06750361172726656)\n",
      "> Loss: 0.1383958041667938\n",
      "------------------------------------------------------------------------\n",
      "************************* SGD --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.7087978720664978 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.8024322390556335 - F1 score: 0.593116819858551\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6917406916618347 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6698154807090759 - F1 score: 0.28333330154418945\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6853622794151306 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.1752900242805481 (+- 0.2359798121457324)\n",
      "> Loss: 0.7116297125816345\n",
      "------------------------------------------------------------------------\n",
      "************************* SGD --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5664064288139343 - F1 score: 0.7570908069610596\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6432794332504272 - F1 score: 0.60663241147995\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6627848744392395 - F1 score: 0.6163502335548401\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6079157590866089 - F1 score: 0.5252489447593689\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6061872839927673 - F1 score: 0.517785906791687\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6046216607093811 (+- 0.08628895560163415)\n",
      "> Loss: 0.6173147559165955\n",
      "------------------------------------------------------------------------\n",
      "************************* SGD --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.3038031756877899 - F1 score: 0.5656403303146362\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2596248686313629 - F1 score: 0.5977091789245605\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.4057775139808655 - F1 score: 0.21843430399894714\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.2454005926847458 - F1 score: 0.13411763310432434\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.22422614693641663 - F1 score: 0.45572441816329956\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.39432517290115354 (+- 0.18608028692733058)\n",
      "> Loss: 0.28776645958423613\n",
      "------------------------------------------------------------------------\n",
      "************************* SGD --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.22190478444099426 - F1 score: 0.5650375485420227\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.22394698858261108 - F1 score: 0.48057135939598083\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.1889791041612625 - F1 score: 0.7423969507217407\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.19901369512081146 - F1 score: 0.6831191778182983\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.23105278611183167 - F1 score: 0.6719469428062439\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6286143958568573 (+- 0.0935453973559527)\n",
      "> Loss: 0.2129794716835022\n",
      "------------------------------------------------------------------------\n",
      "************************* RMSprop --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 1.5065521001815796 - F1 score: 0.898556113243103\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.4505937695503235 - F1 score: 0.8776704668998718\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.46409687399864197 - F1 score: 0.8496677279472351\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.7668123841285706 - F1 score: 0.5444688200950623\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5070288181304932 - F1 score: 0.7744085192680359\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7889543294906616 (+- 0.12927023299285245)\n",
      "> Loss: 0.7390167891979218\n",
      "------------------------------------------------------------------------\n",
      "************************* RMSprop --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.435849130153656 - F1 score: 0.8174055218696594\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.39119580388069153 - F1 score: 0.7424761056900024\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5094019174575806 - F1 score: 0.758202075958252\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.4610884487628937 - F1 score: 0.6148004531860352\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5226844549179077 - F1 score: 0.8240183591842651\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7513805031776428 (+- 0.07538632956186292)\n",
      "> Loss: 0.4640439510345459\n",
      "------------------------------------------------------------------------\n",
      "************************* RMSprop --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.1356833428144455 - F1 score: 0.8402680158615112\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.11118921637535095 - F1 score: 0.8710088729858398\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.25703221559524536 - F1 score: 0.7234162092208862\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.14215658605098724 - F1 score: 0.7484126091003418\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.0898769348859787 - F1 score: 0.6988972425460815\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7764005899429322 (+- 0.067271414201952)\n",
      "> Loss: 0.14718765914440154\n",
      "------------------------------------------------------------------------\n",
      "************************* RMSprop --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.1171010360121727 - F1 score: 0.8174823522567749\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.10684429854154587 - F1 score: 0.8348809480667114\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.12233143299818039 - F1 score: 0.666010320186615\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.18428629636764526 - F1 score: 0.552463710308075\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.11531764268875122 - F1 score: 0.7404001951217651\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7222475051879883 (+- 0.10399485245286401)\n",
      "> Loss: 0.12917614132165908\n",
      "------------------------------------------------------------------------\n",
      "************************* Adadelta --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 2.739372491836548 - F1 score: 0.48099222779273987\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 21.81463623046875 - F1 score: 0.5726481676101685\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 10.375802993774414 - F1 score: 0.6719258427619934\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 76.8987808227539 - F1 score: 0.5000143051147461\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 5.129802227020264 - F1 score: 0.5063943266868591\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5463949739933014 (+- 0.0699656581903421)\n",
      "> Loss: 23.391678953170775\n",
      "------------------------------------------------------------------------\n",
      "************************* Adadelta --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 1.1542744636535645 - F1 score: 0.31644734740257263\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6593708992004395 - F1 score: 0.5782347917556763\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.8752132058143616 - F1 score: 0.3772936761379242\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6858847737312317 - F1 score: 0.2716727554798126\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.7032451629638672 - F1 score: 0.6373808979988098\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.4362058937549591 (+- 0.14527732279985994)\n",
      "> Loss: 0.8155977010726929\n",
      "------------------------------------------------------------------------\n",
      "************************* Adadelta --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.30216217041015625 - F1 score: 0.6041706800460815\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.48541879653930664 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6359890699386597 - F1 score: 0.36083558201789856\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.4518235921859741 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6335386037826538 - F1 score: 0.32849350571632385\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.2586999535560608 (+- 0.23173440889337116)\n",
      "> Loss: 0.5017864465713501\n",
      "------------------------------------------------------------------------\n",
      "************************* Adadelta --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.3024716079235077 - F1 score: 0.4405952990055084\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2600787878036499 - F1 score: 0.2823057472705841\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.29822152853012085 - F1 score: 0.12625309824943542\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.2610204219818115 - F1 score: 0.41989246010780334\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2341345101594925 - F1 score: 0.5240660905838013\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.3586225390434265 (+- 0.13976044867611168)\n",
      "> Loss: 0.2711853712797165\n",
      "------------------------------------------------------------------------\n",
      "************************* Adagrad --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6157472729682922 - F1 score: 0.5352936387062073\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5674309730529785 - F1 score: 0.5952950716018677\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 1.5092501640319824 - F1 score: 0.673524796962738\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.5559938549995422 - F1 score: 0.5277380347251892\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 1.218479037284851 - F1 score: 0.428781658411026\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5521266400814057 (+- 0.0808618135882524)\n",
      "> Loss: 0.8933802604675293\n",
      "------------------------------------------------------------------------\n",
      "************************* Adagrad --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5350059270858765 - F1 score: 0.77010577917099\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5819151401519775 - F1 score: 0.7445298433303833\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6537945866584778 - F1 score: 0.6581432223320007\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.5963603258132935 - F1 score: 0.48667335510253906\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6077905893325806 - F1 score: 0.46759408712387085\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6254092574119567 (+- 0.12676875673506002)\n",
      "> Loss: 0.5949733138084412\n",
      "------------------------------------------------------------------------\n",
      "************************* Adagrad --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.31230175495147705 - F1 score: 0.41244441270828247\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.493959903717041 - F1 score: 0.6693679094314575\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.4469457268714905 - F1 score: 0.12511670589447021\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.5611117482185364 - F1 score: 0.468839555978775\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2535409927368164 - F1 score: 0.7080367803573608\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.4767610728740692 (+- 0.20901626271361193)\n",
      "> Loss: 0.4135720252990723\n",
      "------------------------------------------------------------------------\n",
      "************************* Adagrad --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.2227601855993271 - F1 score: 0.6971734762191772\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.19793394207954407 - F1 score: 0.5229724645614624\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.2238188534975052 - F1 score: 0.45139598846435547\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.21504878997802734 - F1 score: 0.7122159600257874\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.22216513752937317 - F1 score: 0.47015827894210815\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5707832336425781 (+- 0.11193058113902897)\n",
      "> Loss: 0.21634538173675538\n",
      "------------------------------------------------------------------------\n",
      "************************* Adamax --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.4489174783229828 - F1 score: 0.8447058796882629\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.33438965678215027 - F1 score: 0.8814080953598022\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.3492146134376526 - F1 score: 0.8627418279647827\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.3493781089782715 - F1 score: 0.8600640296936035\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.44286927580833435 - F1 score: 0.6706060171127319\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.8239051699638367 (+- 0.07753145859667493)\n",
      "> Loss: 0.3849538266658783\n",
      "------------------------------------------------------------------------\n",
      "************************* Adamax --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.49127593636512756 - F1 score: 0.5981887578964233\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.38237622380256653 - F1 score: 0.8181439638137817\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.4224012792110443 - F1 score: 0.8515582084655762\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.4978170096874237 - F1 score: 0.558111846446991\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.45207351446151733 - F1 score: 0.6947806477546692\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7041566848754883 (+- 0.11607455825319592)\n",
      "> Loss: 0.4491887927055359\n",
      "------------------------------------------------------------------------\n",
      "************************* Adamax --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.3526802659034729 - F1 score: 0.5022221803665161\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.11648038029670715 - F1 score: 0.6556969285011292\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.1352488249540329 - F1 score: 0.7386937141418457\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.1561206579208374 - F1 score: 0.6113077998161316\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.32465076446533203 - F1 score: 0.5532608032226562\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6122362852096558 (+- 0.08180755670793594)\n",
      "> Loss: 0.21703617870807648\n",
      "------------------------------------------------------------------------\n",
      "************************* Adamax --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.1938684731721878 - F1 score: 0.7516036033630371\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.1021437793970108 - F1 score: 0.703754723072052\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.18839946389198303 - F1 score: 0.489784300327301\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.1375703364610672 - F1 score: 0.625927746295929\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.17487990856170654 - F1 score: 0.6803520917892456\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.650284492969513 (+- 0.08986907263752784)\n",
      "> Loss: 0.15937239229679107\n",
      "------------------------------------------------------------------------\n",
      "************************* Nadam --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.4838619530200958 - F1 score: 0.5916229486465454\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 1.4683353900909424 - F1 score: 0.5778271555900574\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.4589238166809082 - F1 score: 0.8458181619644165\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.7211679816246033 - F1 score: 0.6503321528434753\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5090769529342651 - F1 score: 0.8417845964431763\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7014770030975341 (+- 0.1187380180640841)\n",
      "> Loss: 0.7282732188701629\n",
      "------------------------------------------------------------------------\n",
      "************************* Nadam --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5250878930091858 - F1 score: 0.7495755553245544\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.496645450592041 - F1 score: 0.5048484206199646\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.34159496426582336 - F1 score: 0.6591190099716187\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.5336978435516357 - F1 score: 0.6648387908935547\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5983274579048157 - F1 score: 0.7789884805679321\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6714740514755249 (+- 0.09551960213287489)\n",
      "> Loss: 0.4990707218647003\n",
      "------------------------------------------------------------------------\n",
      "************************* Nadam --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.3880443871021271 - F1 score: 0.16129867732524872\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2803140878677368 - F1 score: 0.7065352201461792\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.36172404885292053 - F1 score: 0.4037283957004547\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.21312351524829865 - F1 score: 0.768348753452301\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.3353680968284607 - F1 score: 0.2569999694824219\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.4593822032213211 (+- 0.24060533916438448)\n",
      "> Loss: 0.31571482717990873\n",
      "------------------------------------------------------------------------\n",
      "************************* Nadam --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.12877248227596283 - F1 score: 0.7847694158554077\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.13874512910842896 - F1 score: 0.7543142437934875\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.16279853880405426 - F1 score: 0.697263240814209\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.14438164234161377 - F1 score: 0.6622341275215149\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.16596916317939758 - F1 score: 0.6079212427139282\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7013004541397094 (+- 0.06330826732225033)\n",
      "> Loss: 0.14813339114189147\n",
      "------------------------------------------------------------------------\n",
      "************************* Ftrl --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5733017921447754 - F1 score: 0.5111690759658813\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5415390133857727 - F1 score: 0.6288646459579468\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5660111308097839 - F1 score: 0.6928147077560425\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.4847013056278229 - F1 score: 0.5912643671035767\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5933976173400879 - F1 score: 0.5002695322036743\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5848764657974244 (+- 0.07241207341592311)\n",
      "> Loss: 0.5517901718616486\n",
      "------------------------------------------------------------------------\n",
      "************************* Ftrl --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6441948413848877 - F1 score: 0.5852580070495605\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6339576244354248 - F1 score: 0.6610599756240845\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6265108585357666 - F1 score: 0.4904017448425293\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6203969120979309 - F1 score: 0.6631469130516052\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6076343059539795 - F1 score: 0.4701211452484131\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5739975571632385 (+- 0.08177143619267331)\n",
      "> Loss: 0.6265389084815979\n",
      "------------------------------------------------------------------------\n",
      "************************* Ftrl --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.22372673451900482 - F1 score: 0.6271061301231384\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.21041211485862732 - F1 score: 0.5152730941772461\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.20387572050094604 - F1 score: 0.5044118165969849\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.22876812517642975 - F1 score: 0.6844202280044556\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2594447731971741 - F1 score: 0.35610705614089966\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5374636650085449 (+- 0.11323168165898083)\n",
      "> Loss: 0.2252454936504364\n",
      "------------------------------------------------------------------------\n",
      "************************* Ftrl --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.22944842278957367 - F1 score: 0.5065380334854126\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2370252162218094 - F1 score: 0.4247831404209137\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.23497533798217773 - F1 score: 0.4551559090614319\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.24342191219329834 - F1 score: 0.48003950715065\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.23799027502536774 - F1 score: 0.5533332824707031\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.48396997451782225 (+- 0.04397582528616229)\n",
      "> Loss: 0.23657223284244538\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for opti in optimzers:\n",
    "    for los in lossFunctions:\n",
    "        for act in activationFunctions:\n",
    "            evaluateModel(opti,los,act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(optimizerF, lossF, activationF):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(46, input_dim=46, activation=activationF))\n",
    "    model.add(Dense(20, activation=activationF))\n",
    "    model.add(Dense(10, activation=activationF))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=lossF,optimizer=optimizerF,metrics=[f1_m])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************* adam --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6937194466590881 - F1 score: 0.5532951354980469\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.7249612212181091 - F1 score: 0.751072883605957\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5055830478668213 - F1 score: 0.8266666531562805\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6486217975616455 - F1 score: 0.7351187467575073\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.3218729794025421 - F1 score: 0.6884210109710693\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7109148859977722 (+- 0.09049521911041018)\n",
      "> Loss: 0.5789516985416412\n",
      "------------------------------------------------------------------------\n",
      "************************* adam --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.4795006811618805 - F1 score: 0.6432691812515259\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.49839869141578674 - F1 score: 0.6632825136184692\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.3739428222179413 - F1 score: 0.8334115147590637\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.4964901804924011 - F1 score: 0.8218081593513489\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6064448356628418 - F1 score: 0.5863547325134277\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7096252202987671 (+- 0.09965384292641921)\n",
      "> Loss: 0.4909554421901703\n",
      "------------------------------------------------------------------------\n",
      "************************* adam --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.2027852088212967 - F1 score: 0.7048717141151428\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.11008960753679276 - F1 score: 0.8070744276046753\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.21543839573860168 - F1 score: 0.7439254522323608\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.2078893631696701 - F1 score: 0.8170868158340454\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.25066012144088745 - F1 score: 0.7225505113601685\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7591017842292785 (+- 0.045101786014404566)\n",
      "> Loss: 0.19737253934144974\n",
      "------------------------------------------------------------------------\n",
      "************************* adam --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.13497208058834076 - F1 score: 0.8279821276664734\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.11189240962266922 - F1 score: 0.8474421501159668\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.2008053958415985 - F1 score: 0.7107017040252686\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.18363365530967712 - F1 score: 0.5679940581321716\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.15402209758758545 - F1 score: 0.567827582359314\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7043895244598388 (+- 0.12085687084347338)\n",
      "> Loss: 0.15706512778997422\n",
      "------------------------------------------------------------------------\n",
      "************************* SGD --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6731695532798767 - F1 score: 0.6287411451339722\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6963021755218506 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6268802881240845 - F1 score: 0.20476189255714417\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6701446771621704 - F1 score: 0.16574013233184814\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.7189348340034485 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.1998486340045929 (+- 0.23022392105332237)\n",
      "> Loss: 0.6770863056182861\n",
      "------------------------------------------------------------------------\n",
      "************************* SGD --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6103107929229736 - F1 score: 0.7085423469543457\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.578595757484436 - F1 score: 0.5428308248519897\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5567493438720703 - F1 score: 0.7694699764251709\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6276522278785706 - F1 score: 0.6563588380813599\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6249206066131592 - F1 score: 0.4680422842502594\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6290488541126251 (+- 0.10967206237551731)\n",
      "> Loss: 0.5996457457542419\n",
      "------------------------------------------------------------------------\n",
      "************************* SGD --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.2639811933040619 - F1 score: 0.627409040927887\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.21271267533302307 - F1 score: 0.6062186360359192\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.19024783372879028 - F1 score: 0.772335946559906\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.3558996319770813 - F1 score: 0.19191917777061462\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.38531550765037537 - F1 score: 0.17086832225322723\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.4737502247095108 (+- 0.24555184012672712)\n",
      "> Loss: 0.28163136839866637\n",
      "------------------------------------------------------------------------\n",
      "************************* SGD --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.2398502081632614 - F1 score: 0.6502221822738647\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.20643287897109985 - F1 score: 0.5482620596885681\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.23515823483467102 - F1 score: 0.4498108923435211\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.17325396835803986 - F1 score: 0.6034208536148071\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.20744195580482483 - F1 score: 0.45186877250671387\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.540716952085495 (+- 0.08017233196885586)\n",
      "> Loss: 0.2124274492263794\n",
      "------------------------------------------------------------------------\n",
      "************************* RMSprop --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.541918158531189 - F1 score: 0.7233666181564331\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.39816227555274963 - F1 score: 0.6612626314163208\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.8305990099906921 - F1 score: 0.7609612941741943\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6478942036628723 - F1 score: 0.6454057693481445\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.44855552911758423 - F1 score: 0.770896315574646\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7123785257339478 (+- 0.05099713725700057)\n",
      "> Loss: 0.5734258353710174\n",
      "------------------------------------------------------------------------\n",
      "************************* RMSprop --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5154917240142822 - F1 score: 0.6334190368652344\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5419797897338867 - F1 score: 0.8022710084915161\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.4998004138469696 - F1 score: 0.8721969723701477\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.4334148168563843 - F1 score: 0.6332234144210815\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.33937206864356995 - F1 score: 0.852643609046936\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7587508082389831 (+- 0.10492385576603046)\n",
      "> Loss: 0.46601176261901855\n",
      "------------------------------------------------------------------------\n",
      "************************* RMSprop --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.1423683911561966 - F1 score: 0.6453403234481812\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2691361904144287 - F1 score: 0.33077341318130493\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.2780194878578186 - F1 score: 0.5585857629776001\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.1460852473974228 - F1 score: 0.6430966258049011\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.27411213517189026 - F1 score: 0.5145863890647888\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5384765028953552 (+- 0.11530909731448831)\n",
      "> Loss: 0.22194429039955138\n",
      "------------------------------------------------------------------------\n",
      "************************* RMSprop --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.15562304854393005 - F1 score: 0.7586013078689575\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.12278979271650314 - F1 score: 0.7716706991195679\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.15377679467201233 - F1 score: 0.602290153503418\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.11900129169225693 - F1 score: 0.7919726967811584\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.1811237335205078 - F1 score: 0.7989338636398315\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7446937441825867 (+- 0.07263229169073049)\n",
      "> Loss: 0.14646293222904205\n",
      "------------------------------------------------------------------------\n",
      "************************* Adadelta --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 29.560867309570312 - F1 score: 0.18410369753837585\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 8.835984230041504 - F1 score: 0.5672499537467957\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 125.37932586669922 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 24.624034881591797 - F1 score: 0.5372824668884277\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 2.820340156555176 - F1 score: 0.4934941232204437\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.3564260482788086 (+- 0.22480121925542776)\n",
      "> Loss: 38.2441104888916\n",
      "------------------------------------------------------------------------\n",
      "************************* Adadelta --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.8312585353851318 - F1 score: 0.39336317777633667\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 1.0197272300720215 - F1 score: 0.2289884090423584\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.8444838523864746 - F1 score: 0.5499433279037476\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6942086219787598 - F1 score: 0.5900610089302063\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.7911202311515808 - F1 score: 0.2028985321521759\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.393050891160965 (+- 0.15906145881953557)\n",
      "> Loss: 0.8361596941947937\n",
      "------------------------------------------------------------------------\n",
      "************************* Adadelta --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.49309447407722473 - F1 score: 0.6928700804710388\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5919715166091919 - F1 score: 0.3854212164878845\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5432403087615967 - F1 score: 0.6856393218040466\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.5462560057640076 - F1 score: 0.620026707649231\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.3379446268081665 - F1 score: 0.09019607305526733\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.49483067989349366 (+- 0.231084912146383)\n",
      "> Loss: 0.5025013864040375\n",
      "------------------------------------------------------------------------\n",
      "************************* Adadelta --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.3170625567436218 - F1 score: 0.23943698406219482\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.27030134201049805 - F1 score: 0.42437902092933655\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.2737884223461151 - F1 score: 0.5004372596740723\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.2576495409011841 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2566128373146057 - F1 score: 0.4172670245170593\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.3163040578365326 (+- 0.17985302796554428)\n",
      "> Loss: 0.275082939863205\n",
      "------------------------------------------------------------------------\n",
      "************************* Adagrad --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.7632824778556824 - F1 score: 0.6060105562210083\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.615394651889801 - F1 score: 0.42359861731529236\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5309121012687683 - F1 score: 0.7597240805625916\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.7694337964057922 - F1 score: 0.39891061186790466\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.503463864326477 - F1 score: 0.5946838855743408\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5565855503082275 (+- 0.1324435633713151)\n",
      "> Loss: 0.6364973783493042\n",
      "------------------------------------------------------------------------\n",
      "************************* Adagrad --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6131207942962646 - F1 score: 0.43549782037734985\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6011880040168762 - F1 score: 0.4978996217250824\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6356881260871887 - F1 score: 0.6314622163772583\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6086010932922363 - F1 score: 0.687293529510498\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5724652409553528 - F1 score: 0.5660426616668701\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5636391699314117 (+- 0.09015324365605537)\n",
      "> Loss: 0.6062126517295837\n",
      "------------------------------------------------------------------------\n",
      "************************* Adagrad --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.2362961769104004 - F1 score: 0.6158740520477295\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.18352553248405457 - F1 score: 0.8112101554870605\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.20529887080192566 - F1 score: 0.5895237326622009\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.22631970047950745 - F1 score: 0.4853001534938812\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2212754637002945 - F1 score: 0.5452540516853333\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6094324290752411 (+- 0.11016332197307546)\n",
      "> Loss: 0.2145431488752365\n",
      "------------------------------------------------------------------------\n",
      "************************* Adagrad --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.21666103601455688 - F1 score: 0.6088517904281616\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.19051040709018707 - F1 score: 0.7478762269020081\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.22518746554851532 - F1 score: 0.5655128955841064\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.2160821408033371 - F1 score: 0.4363612234592438\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.209079772233963 - F1 score: 0.6968109011650085\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6110826075077057 (+- 0.10832448649667197)\n",
      "> Loss: 0.21150416433811187\n",
      "------------------------------------------------------------------------\n",
      "************************* Adamax --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5084077715873718 - F1 score: 0.6979328393936157\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6277748346328735 - F1 score: 0.8248767852783203\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.3883211612701416 - F1 score: 0.7736608982086182\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.3730270564556122 - F1 score: 0.8136337995529175\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.4377400279045105 - F1 score: 0.8507388830184937\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7921686410903931 (+- 0.05326156556386225)\n",
      "> Loss: 0.46705417037010194\n",
      "------------------------------------------------------------------------\n",
      "************************* Adamax --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.46000221371650696 - F1 score: 0.5859934687614441\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5906030535697937 - F1 score: 0.7079614996910095\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.4483415484428406 - F1 score: 0.784138560295105\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.46049824357032776 - F1 score: 0.8038710355758667\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6069017052650452 - F1 score: 0.7051002383232117\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7174129605293273 (+- 0.07673241878305119)\n",
      "> Loss: 0.5132693529129029\n",
      "------------------------------------------------------------------------\n",
      "************************* Adamax --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.36283883452415466 - F1 score: 0.38104623556137085\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.36713501811027527 - F1 score: 0.33822116255760193\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.1776399165391922 - F1 score: 0.6051141619682312\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.1513298749923706 - F1 score: 0.6068315505981445\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.15025928616523743 - F1 score: 0.6428481340408325\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5148122489452363 (+- 0.12813503297123047)\n",
      "> Loss: 0.24184058606624603\n",
      "------------------------------------------------------------------------\n",
      "************************* Adamax --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.11033879220485687 - F1 score: 0.831336498260498\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.1302146315574646 - F1 score: 0.673168420791626\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.13400112092494965 - F1 score: 0.8374879956245422\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.1252446174621582 - F1 score: 0.6503167152404785\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.14699389040470123 - F1 score: 0.810193657875061\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7605006575584412 (+- 0.08146358886565468)\n",
      "> Loss: 0.12935861051082612\n",
      "------------------------------------------------------------------------\n",
      "************************* Nadam --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5110711455345154 - F1 score: 0.7429142594337463\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.36101633310317993 - F1 score: 0.8536596298217773\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6230981349945068 - F1 score: 0.6020113229751587\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.5129848122596741 - F1 score: 0.8487296104431152\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5642539858818054 - F1 score: 0.6012130379676819\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7297055721282959 (+- 0.11182303173995929)\n",
      "> Loss: 0.5144848823547363\n",
      "------------------------------------------------------------------------\n",
      "************************* Nadam --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5280428528785706 - F1 score: 0.8021871447563171\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.4715474247932434 - F1 score: 0.5887348651885986\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.4860936403274536 - F1 score: 0.813930869102478\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.5566326379776001 - F1 score: 0.7629729509353638\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5303338766098022 - F1 score: 0.5864468812942505\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7108545422554016 (+- 0.10205212745644462)\n",
      "> Loss: 0.514530086517334\n",
      "------------------------------------------------------------------------\n",
      "************************* Nadam --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.20942696928977966 - F1 score: 0.7178719639778137\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.21686233580112457 - F1 score: 0.7279439568519592\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.23719783127307892 - F1 score: 0.3660130500793457\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.16853676736354828 - F1 score: 0.8580265045166016\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.15117882192134857 - F1 score: 0.6461859941482544\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.663208293914795 (+- 0.16355288423657757)\n",
      "> Loss: 0.196640545129776\n",
      "------------------------------------------------------------------------\n",
      "************************* Nadam --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.12162128835916519 - F1 score: 0.5887795090675354\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.11613520979881287 - F1 score: 0.8358974456787109\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.15432506799697876 - F1 score: 0.6486255526542664\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.18817462027072906 - F1 score: 0.5772875547409058\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.17698000371456146 - F1 score: 0.7766233086585999\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6854426741600037 (+- 0.10329312093119689)\n",
      "> Loss: 0.15144723802804946\n",
      "------------------------------------------------------------------------\n",
      "************************* Ftrl --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6122868061065674 - F1 score: 0.6553332805633545\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5323411226272583 - F1 score: 0.6502631902694702\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5749737620353699 - F1 score: 0.7245420217514038\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6536314487457275 - F1 score: 0.4597979485988617\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5331785082817078 - F1 score: 0.5007239580154419\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5981320798397064 (+- 0.10059202504818761)\n",
      "> Loss: 0.5812823295593261\n",
      "------------------------------------------------------------------------\n",
      "************************* Ftrl --- binary_crossentropy --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6922395825386047 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.692142128944397 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6924353837966919 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6921847462654114 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6912308931350708 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.0 (+- 0.0)\n",
      "> Loss: 0.6920465469360352\n",
      "------------------------------------------------------------------------\n",
      "************************* Ftrl --- mean_squared_error --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.2491942197084427 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.20875322818756104 - F1 score: 0.6016871333122253\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.23340344429016113 - F1 score: 0.5565268993377686\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.24963469803333282 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2491757571697235 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.23164280652999877 (+- 0.2840625448715554)\n",
      "> Loss: 0.23803226947784423\n",
      "------------------------------------------------------------------------\n",
      "************************* Ftrl --- mean_squared_error --- tanh****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.24917712807655334 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2496025115251541 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.24942028522491455 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.250179260969162 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.24929727613925934 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.0 (+- 0.0)\n",
      "> Loss: 0.24953529238700867\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for opti in optimzers:\n",
    "    for los in lossFunctions:\n",
    "        for act in activationFunctions:\n",
    "            evaluateModel(opti,los,act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************* Adamax --- binary_crossentropy --- relu****************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.433227002620697 - F1 score: 0.8801926374435425\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5351843237876892 - F1 score: 0.8434548377990723\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6507638692855835 - F1 score: 0.5497072339057922\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.4321388006210327 - F1 score: 0.6746543645858765\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.3839220404624939 - F1 score: 0.8766530752182007\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7649324297904968 (+- 0.13145660929992853)\n",
      "> Loss: 0.4870472073554993\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from keras.regularizers import l2,l1\n",
    "\n",
    "def get_model(optimizerF, lossF, activationF):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(46, input_dim=46, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu', kernel_regularizer=l1(0.002), bias_regularizer=l1(0.002)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=lossF,optimizer=optimizerF,metrics=[f1_m])\n",
    "    return model\n",
    "evaluateModel('Adamax','binary_crossentropy','relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
